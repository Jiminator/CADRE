job is starting on gpua021.delta.ncsa.illinois.edu
Using fixed seeds: 5497 58475 94707
alpha = 0.0
SEED: 5497
Loading drug dataset...
Hyperparameters:
Namespace(seed=5497, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.3, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=True, alpha=0.0, gamma=2.0, adam=False, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING SGD OPTIMIZER
USING FOCAL LOSS
GAMMA: 2.0
ALPHA: tensor(0.)
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:90.0, f1:0.0, auc:65.1 | trn acc:67.8, f1:0.2, auc:61.6 | loss:0.001
Average epoch runtime: 4.05 seconds
Total training time: 291.38 GPU seconds
SEED: 58475
Loading drug dataset...
Hyperparameters:
Namespace(seed=58475, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.3, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=True, alpha=0.0, gamma=2.0, adam=False, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING SGD OPTIMIZER
USING FOCAL LOSS
GAMMA: 2.0
ALPHA: tensor(0.)
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:71.4, f1:0.0, auc:58.4 | trn acc:69.2, f1:0.1, auc:61.3 | loss:0.002
Average epoch runtime: 4.04 seconds
Total training time: 291.15 GPU seconds
SEED: 94707
Loading drug dataset...
Hyperparameters:
Namespace(seed=94707, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.3, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=True, alpha=0.0, gamma=2.0, adam=False, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING SGD OPTIMIZER
USING FOCAL LOSS
GAMMA: 2.0
ALPHA: tensor(0.)
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:50.6, f1:0.0, auc:56.5 | trn acc:69.2, f1:0.1, auc:62.2 | loss:0.001
Average epoch runtime: 4.04 seconds
Total training time: 291.21 GPU seconds
alpha = 0.1
SEED: 5497
Loading drug dataset...
Hyperparameters:
Namespace(seed=5497, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.3, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=True, alpha=0.1, gamma=2.0, adam=False, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING SGD OPTIMIZER
USING FOCAL LOSS
GAMMA: 2.0
ALPHA: tensor(0.1000)
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:89.1, f1:21.9, auc:81.6 | trn acc:70.9, f1:19.9, auc:82.3 | loss:0.030
Average epoch runtime: 4.04 seconds
Total training time: 291.10 GPU seconds
SEED: 58475
Loading drug dataset...
Hyperparameters:
Namespace(seed=58475, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.3, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=True, alpha=0.1, gamma=2.0, adam=False, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING SGD OPTIMIZER
USING FOCAL LOSS
GAMMA: 2.0
ALPHA: tensor(0.1000)
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:74.0, f1:20.6, auc:81.1 | trn acc:72.1, f1:19.6, auc:82.3 | loss:0.029
Average epoch runtime: 4.05 seconds
Total training time: 291.26 GPU seconds
SEED: 94707
Loading drug dataset...
Hyperparameters:
Namespace(seed=94707, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.3, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=True, alpha=0.1, gamma=2.0, adam=False, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING SGD OPTIMIZER
USING FOCAL LOSS
GAMMA: 2.0
ALPHA: tensor(0.1000)
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:52.0, f1:6.6, auc:71.5 | trn acc:72.2, f1:19.7, auc:82.5 | loss:0.029
Average epoch runtime: 4.04 seconds
Total training time: 291.12 GPU seconds
alpha = 0.2
SEED: 5497
Loading drug dataset...
Hyperparameters:
Namespace(seed=5497, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.3, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=True, alpha=0.2, gamma=2.0, adam=False, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING SGD OPTIMIZER
USING FOCAL LOSS
GAMMA: 2.0
ALPHA: tensor(0.2000)
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:86.9, f1:37.5, auc:82.7 | trn acc:74.8, f1:41.1, auc:83.5 | loss:0.043
Average epoch runtime: 4.04 seconds
Total training time: 291.24 GPU seconds
SEED: 58475
Loading drug dataset...
Hyperparameters:
Namespace(seed=58475, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.3, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=True, alpha=0.2, gamma=2.0, adam=False, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING SGD OPTIMIZER
USING FOCAL LOSS
GAMMA: 2.0
ALPHA: tensor(0.2000)
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:76.4, f1:38.7, auc:82.9 | trn acc:75.6, f1:40.0, auc:83.6 | loss:0.042
Average epoch runtime: 4.04 seconds
Total training time: 291.04 GPU seconds
SEED: 94707
Loading drug dataset...
Hyperparameters:
Namespace(seed=94707, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.3, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=True, alpha=0.2, gamma=2.0, adam=False, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING SGD OPTIMIZER
USING FOCAL LOSS
GAMMA: 2.0
ALPHA: tensor(0.2000)
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:55.1, f1:23.2, auc:72.8 | trn acc:75.7, f1:40.1, auc:83.7 | loss:0.042
Average epoch runtime: 4.04 seconds
Total training time: 291.23 GPU seconds
alpha = 0.3
SEED: 5497
Loading drug dataset...
Hyperparameters:
Namespace(seed=5497, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.3, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=True, alpha=0.3, gamma=2.0, adam=False, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING SGD OPTIMIZER
USING FOCAL LOSS
GAMMA: 2.0
ALPHA: tensor(0.3000)
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:82.3, f1:37.2, auc:82.7 | trn acc:78.0, f1:55.3, auc:84.0 | loss:0.051
Average epoch runtime: 4.04 seconds
Total training time: 291.18 GPU seconds
SEED: 58475
Loading drug dataset...
Hyperparameters:
Namespace(seed=58475, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.3, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=True, alpha=0.3, gamma=2.0, adam=False, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING SGD OPTIMIZER
USING FOCAL LOSS
GAMMA: 2.0
ALPHA: tensor(0.3000)
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:77.6, f1:51.8, auc:83.5 | trn acc:78.6, f1:54.4, auc:84.1 | loss:0.050
Average epoch runtime: 4.04 seconds
Total training time: 291.13 GPU seconds
SEED: 94707
Loading drug dataset...
Hyperparameters:
Namespace(seed=94707, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.3, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=True, alpha=0.3, gamma=2.0, adam=False, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING SGD OPTIMIZER
USING FOCAL LOSS
GAMMA: 2.0
ALPHA: tensor(0.3000)
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:57.9, f1:34.9, auc:73.5 | trn acc:78.6, f1:54.3, auc:84.3 | loss:0.050
Average epoch runtime: 4.05 seconds
Total training time: 291.29 GPU seconds
alpha = 0.4
SEED: 5497
Loading drug dataset...
Hyperparameters:
Namespace(seed=5497, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.3, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=True, alpha=0.4, gamma=2.0, adam=False, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING SGD OPTIMIZER
USING FOCAL LOSS
GAMMA: 2.0
ALPHA: tensor(0.4000)
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:79.5, f1:39.7, auc:82.3 | trn acc:79.8, f1:63.3, auc:84.3 | loss:0.056
Average epoch runtime: 4.07 seconds
Total training time: 292.95 GPU seconds
SEED: 58475
Loading drug dataset...
Hyperparameters:
Namespace(seed=58475, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.3, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=True, alpha=0.4, gamma=2.0, adam=False, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING SGD OPTIMIZER
USING FOCAL LOSS
GAMMA: 2.0
ALPHA: tensor(0.4000)
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:78.1, f1:56.9, auc:83.8 | trn acc:80.4, f1:62.7, auc:84.5 | loss:0.055
Average epoch runtime: 4.04 seconds
Total training time: 291.09 GPU seconds
SEED: 94707
Loading drug dataset...
Hyperparameters:
Namespace(seed=94707, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.3, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=True, alpha=0.4, gamma=2.0, adam=False, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING SGD OPTIMIZER
USING FOCAL LOSS
GAMMA: 2.0
ALPHA: tensor(0.4000)
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:60.5, f1:42.6, auc:73.9 | trn acc:80.4, f1:62.6, auc:84.6 | loss:0.055
Average epoch runtime: 4.04 seconds
Total training time: 291.11 GPU seconds
alpha = 0.5
SEED: 5497
Loading drug dataset...
Hyperparameters:
Namespace(seed=5497, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.3, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=True, alpha=0.5, gamma=2.0, adam=False, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING SGD OPTIMIZER
USING FOCAL LOSS
GAMMA: 2.0
ALPHA: tensor(0.5000)
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:76.0, f1:39.6, auc:81.6 | trn acc:80.5, f1:67.5, auc:84.5 | loss:0.058
Average epoch runtime: 4.04 seconds
Total training time: 291.19 GPU seconds
SEED: 58475
Loading drug dataset...
Hyperparameters:
Namespace(seed=58475, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.3, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=True, alpha=0.5, gamma=2.0, adam=False, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING SGD OPTIMIZER
USING FOCAL LOSS
GAMMA: 2.0
ALPHA: tensor(0.5000)
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:78.4, f1:61.9, auc:83.8 | trn acc:80.8, f1:66.5, auc:84.6 | loss:0.057
Average epoch runtime: 4.05 seconds
Total training time: 291.61 GPU seconds
SEED: 94707
Loading drug dataset...
Hyperparameters:
Namespace(seed=94707, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.3, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=True, alpha=0.5, gamma=2.0, adam=False, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING SGD OPTIMIZER
USING FOCAL LOSS
GAMMA: 2.0
ALPHA: tensor(0.5000)
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:64.1, f1:53.1, auc:73.9 | trn acc:80.9, f1:66.7, auc:84.8 | loss:0.057
Average epoch runtime: 4.05 seconds
Total training time: 291.87 GPU seconds
alpha = 0.6
SEED: 5497
Loading drug dataset...
Hyperparameters:
Namespace(seed=5497, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.3, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=True, alpha=0.6, gamma=2.0, adam=False, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING SGD OPTIMIZER
USING FOCAL LOSS
GAMMA: 2.0
ALPHA: tensor(0.6000)
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:72.9, f1:38.0, auc:81.1 | trn acc:79.6, f1:68.7, auc:84.6 | loss:0.058
Average epoch runtime: 4.07 seconds
Total training time: 293.11 GPU seconds
SEED: 58475
Loading drug dataset...
Hyperparameters:
Namespace(seed=58475, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.3, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=True, alpha=0.6, gamma=2.0, adam=False, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING SGD OPTIMIZER
USING FOCAL LOSS
GAMMA: 2.0
ALPHA: tensor(0.6000)
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:77.6, f1:64.4, auc:83.7 | trn acc:79.9, f1:67.9, auc:84.6 | loss:0.057
Average epoch runtime: 4.05 seconds
Total training time: 291.57 GPU seconds
SEED: 94707
Loading drug dataset...
Hyperparameters:
Namespace(seed=94707, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.3, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=True, alpha=0.6, gamma=2.0, adam=False, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING SGD OPTIMIZER
USING FOCAL LOSS
GAMMA: 2.0
ALPHA: tensor(0.6000)
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:67.8, f1:61.0, auc:74.1 | trn acc:80.2, f1:68.3, auc:84.8 | loss:0.057
Average epoch runtime: 4.04 seconds
Total training time: 291.10 GPU seconds
alpha = 0.7
SEED: 5497
Loading drug dataset...
Hyperparameters:
Namespace(seed=5497, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.3, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=True, alpha=0.7, gamma=2.0, adam=False, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING SGD OPTIMIZER
USING FOCAL LOSS
GAMMA: 2.0
ALPHA: tensor(0.7000)
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:64.0, f1:32.1, auc:80.4 | trn acc:77.0, f1:68.5, auc:84.5 | loss:0.055
Average epoch runtime: 4.04 seconds
Total training time: 291.16 GPU seconds
SEED: 58475
Loading drug dataset...
Hyperparameters:
Namespace(seed=58475, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.3, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=True, alpha=0.7, gamma=2.0, adam=False, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING SGD OPTIMIZER
USING FOCAL LOSS
GAMMA: 2.0
ALPHA: tensor(0.7000)
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:76.2, f1:66.2, auc:83.5 | trn acc:77.1, f1:67.4, auc:84.6 | loss:0.054
Average epoch runtime: 4.04 seconds
Total training time: 291.14 GPU seconds
SEED: 94707
Loading drug dataset...
Hyperparameters:
Namespace(seed=94707, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.3, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=True, alpha=0.7, gamma=2.0, adam=False, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING SGD OPTIMIZER
USING FOCAL LOSS
GAMMA: 2.0
ALPHA: tensor(0.7000)
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:68.6, f1:64.3, auc:74.4 | trn acc:77.3, f1:67.6, auc:84.7 | loss:0.054
Average epoch runtime: 4.04 seconds
Total training time: 291.03 GPU seconds
alpha = 0.8
SEED: 5497
Loading drug dataset...
Hyperparameters:
Namespace(seed=5497, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.3, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=True, alpha=0.8, gamma=2.0, adam=False, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING SGD OPTIMIZER
USING FOCAL LOSS
GAMMA: 2.0
ALPHA: tensor(0.8000)
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:56.1, f1:30.4, auc:79.5 | trn acc:71.4, f1:65.9, auc:84.3 | loss:0.047
Average epoch runtime: 4.05 seconds
Total training time: 291.25 GPU seconds
SEED: 58475
Loading drug dataset...
Hyperparameters:
Namespace(seed=58475, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.3, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=True, alpha=0.8, gamma=2.0, adam=False, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING SGD OPTIMIZER
USING FOCAL LOSS
GAMMA: 2.0
ALPHA: tensor(0.8000)
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:68.0, f1:61.7, auc:82.9 | trn acc:71.2, f1:64.7, auc:84.3 | loss:0.047
Average epoch runtime: 4.04 seconds
Total training time: 291.11 GPU seconds
SEED: 94707
Loading drug dataset...
Hyperparameters:
Namespace(seed=94707, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.3, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=True, alpha=0.8, gamma=2.0, adam=False, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING SGD OPTIMIZER
USING FOCAL LOSS
GAMMA: 2.0
ALPHA: tensor(0.8000)
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:67.5, f1:67.4, auc:74.7 | trn acc:71.4, f1:64.8, auc:84.4 | loss:0.047
Average epoch runtime: 4.05 seconds
Total training time: 291.28 GPU seconds
alpha = 0.9
SEED: 5497
Loading drug dataset...
Hyperparameters:
Namespace(seed=5497, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.3, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=True, alpha=0.9, gamma=2.0, adam=False, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING SGD OPTIMIZER
USING FOCAL LOSS
GAMMA: 2.0
ALPHA: tensor(0.9000)
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:39.7, f1:24.6, auc:78.4 | trn acc:60.0, f1:60.1, auc:83.7 | loss:0.034
Average epoch runtime: 4.04 seconds
Total training time: 291.21 GPU seconds
SEED: 58475
Loading drug dataset...
Hyperparameters:
Namespace(seed=58475, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.3, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=True, alpha=0.9, gamma=2.0, adam=False, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING SGD OPTIMIZER
USING FOCAL LOSS
GAMMA: 2.0
ALPHA: tensor(0.9000)
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:55.0, f1:55.6, auc:81.6 | trn acc:59.6, f1:58.8, auc:83.6 | loss:0.034
Average epoch runtime: 4.04 seconds
Total training time: 291.13 GPU seconds
SEED: 94707
Loading drug dataset...
Hyperparameters:
Namespace(seed=94707, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.3, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=True, alpha=0.9, gamma=2.0, adam=False, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING SGD OPTIMIZER
USING FOCAL LOSS
GAMMA: 2.0
ALPHA: tensor(0.9000)
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:67.2, f1:72.1, auc:74.7 | trn acc:59.4, f1:58.7, auc:83.7 | loss:0.034
Average epoch runtime: 4.04 seconds
Total training time: 291.12 GPU seconds
alpha = 1.0
SEED: 5497
Loading drug dataset...
Hyperparameters:
Namespace(seed=5497, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.3, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=True, alpha=1.0, gamma=2.0, adam=False, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING SGD OPTIMIZER
USING FOCAL LOSS
GAMMA: 2.0
ALPHA: tensor(1.)
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:10.5, f1:18.3, auc:67.7 | trn acc:32.7, f1:48.9, auc:69.9 | loss:0.001
Average epoch runtime: 4.05 seconds
Total training time: 291.26 GPU seconds
SEED: 58475
Loading drug dataset...
Hyperparameters:
Namespace(seed=58475, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.3, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=True, alpha=1.0, gamma=2.0, adam=False, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING SGD OPTIMIZER
USING FOCAL LOSS
GAMMA: 2.0
ALPHA: tensor(1.)
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:29.3, f1:44.7, auc:66.0 | trn acc:31.5, f1:47.4, auc:69.6 | loss:0.001
Average epoch runtime: 4.04 seconds
Total training time: 291.17 GPU seconds
SEED: 94707
Loading drug dataset...
Hyperparameters:
Namespace(seed=94707, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.3, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=True, alpha=1.0, gamma=2.0, adam=False, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING SGD OPTIMIZER
USING FOCAL LOSS
GAMMA: 2.0
ALPHA: tensor(1.)
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:50.0, f1:66.3, auc:62.1 | trn acc:31.3, f1:47.3, auc:69.3 | loss:0.001
Average epoch runtime: 4.06 seconds
Total training time: 292.42 GPU seconds
