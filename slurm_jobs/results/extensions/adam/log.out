job is starting on gpua086.delta.ncsa.illinois.edu
Using fixed seeds: 5497 58475 94707
LR = 0.0001 using OneCycleSEED: 5497
Loading drug dataset...
Hyperparameters:
Namespace(seed=5497, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.0001, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=False, alpha=0.6, gamma=2.0, adam=True, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING ADAM OPTIMIZER
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
[0,0] | tst acc:51.7, f1:21.4, auc:62.3 | trn acc:52.0, f1:41.1, auc:53.9 | loss:4.221
[1,348] | tst acc:47.6, f1:16.7, auc:51.2 | trn acc:51.4, f1:41.5, auc:53.3 | loss:4.140
[3,20] | tst acc:49.3, f1:19.4, auc:55.3 | trn acc:51.5, f1:42.1, auc:53.4 | loss:4.133
[4,368] | tst acc:50.0, f1:19.1, auc:59.3 | trn acc:51.8, f1:42.6, auc:53.7 | loss:4.068
[6,40] | tst acc:50.2, f1:21.4, auc:59.0 | trn acc:52.0, f1:42.1, auc:53.9 | loss:4.007
[7,388] | tst acc:52.8, f1:18.2, auc:52.5 | trn acc:52.2, f1:42.7, auc:54.2 | loss:3.918
[9,60] | tst acc:48.3, f1:19.7, auc:55.7 | trn acc:52.5, f1:42.7, auc:54.6 | loss:3.783
[10,408] | tst acc:47.4, f1:20.5, auc:57.5 | trn acc:52.7, f1:43.0, auc:55.1 | loss:3.629
[12,80] | tst acc:50.2, f1:17.4, auc:57.2 | trn acc:53.0, f1:43.4, auc:55.6 | loss:3.476
[13,428] | tst acc:50.9, f1:21.6, auc:60.9 | trn acc:53.3, f1:44.0, auc:56.1 | loss:3.317
[15,100] | tst acc:53.1, f1:22.9, auc:61.6 | trn acc:54.0, f1:44.3, auc:56.9 | loss:3.170
[16,448] | tst acc:53.9, f1:21.0, auc:56.7 | trn acc:54.4, f1:44.4, auc:57.3 | loss:3.037
[18,120] | tst acc:50.9, f1:20.5, auc:60.4 | trn acc:54.9, f1:45.0, auc:58.1 | loss:2.892
[19,468] | tst acc:49.6, f1:15.4, auc:48.2 | trn acc:55.3, f1:44.8, auc:58.2 | loss:2.777
[21,140] | tst acc:55.0, f1:20.8, auc:58.6 | trn acc:56.3, f1:45.9, auc:59.2 | loss:2.626
[22,488] | tst acc:54.4, f1:20.5, auc:58.1 | trn acc:56.9, f1:45.9, auc:59.9 | loss:2.515
[24,160] | tst acc:55.2, f1:24.4, auc:65.1 | trn acc:57.5, f1:46.4, auc:60.6 | loss:2.398
[25,508] | tst acc:54.8, f1:21.9, auc:63.1 | trn acc:58.4, f1:47.1, auc:61.5 | loss:2.287
[27,180] | tst acc:54.1, f1:21.6, auc:63.0 | trn acc:59.1, f1:47.2, auc:62.1 | loss:2.203
[28,528] | tst acc:54.1, f1:19.2, auc:60.7 | trn acc:60.1, f1:48.0, auc:63.1 | loss:2.091
[30,200] | tst acc:59.0, f1:22.3, auc:61.3 | trn acc:60.5, f1:48.2, auc:63.7 | loss:2.006
[31,548] | tst acc:58.3, f1:22.0, auc:63.3 | trn acc:61.4, f1:48.7, auc:64.5 | loss:1.913
[33,220] | tst acc:60.7, f1:26.2, auc:71.3 | trn acc:62.1, f1:49.5, auc:65.3 | loss:1.783
[34,568] | tst acc:60.5, f1:27.3, auc:73.6 | trn acc:62.2, f1:49.5, auc:64.5 | loss:1.135
[36,240] | tst acc:57.4, f1:24.1, auc:67.6 | trn acc:62.5, f1:49.3, auc:65.2 | loss:0.682
[37,588] | tst acc:59.2, f1:23.0, auc:67.2 | trn acc:62.8, f1:49.9, auc:65.3 | loss:0.681
[39,260] | tst acc:61.8, f1:29.7, auc:70.3 | trn acc:63.1, f1:49.9, auc:65.8 | loss:0.675
[40,608] | tst acc:59.8, f1:25.8, auc:69.5 | trn acc:63.3, f1:50.3, auc:66.0 | loss:0.667
[42,280] | tst acc:67.7, f1:28.2, auc:70.3 | trn acc:63.3, f1:50.0, auc:66.1 | loss:0.668
[43,628] | tst acc:60.3, f1:24.8, auc:65.7 | trn acc:63.8, f1:50.4, auc:66.6 | loss:0.663
[45,300] | tst acc:63.3, f1:25.7, auc:68.3 | trn acc:64.0, f1:50.6, auc:66.8 | loss:0.668
[46,648] | tst acc:60.9, f1:27.5, auc:68.2 | trn acc:64.1, f1:50.9, auc:67.2 | loss:0.657
[48,320] | tst acc:60.0, f1:24.1, auc:66.5 | trn acc:64.5, f1:50.7, auc:67.3 | loss:0.663
[49,668] | tst acc:64.0, f1:24.7, auc:64.6 | trn acc:64.6, f1:51.3, auc:67.6 | loss:0.652
[51,340] | tst acc:65.7, f1:29.6, auc:73.6 | trn acc:64.7, f1:51.1, auc:67.7 | loss:0.652
[53,12] | tst acc:60.9, f1:25.1, auc:67.6 | trn acc:64.7, f1:51.2, auc:67.9 | loss:0.656
[54,360] | tst acc:60.7, f1:23.1, auc:62.1 | trn acc:65.0, f1:51.2, auc:68.1 | loss:0.654
[56,32] | tst acc:62.7, f1:26.0, auc:68.4 | trn acc:65.1, f1:51.6, auc:68.2 | loss:0.647
[57,380] | tst acc:62.4, f1:27.7, auc:69.9 | trn acc:65.0, f1:51.4, auc:68.2 | loss:0.650
[59,52] | tst acc:60.5, f1:24.9, auc:68.2 | trn acc:65.3, f1:51.7, auc:68.5 | loss:0.647
[60,400] | tst acc:61.4, f1:27.2, auc:71.9 | trn acc:65.5, f1:51.7, auc:68.6 | loss:0.645
[62,72] | tst acc:62.4, f1:25.2, auc:66.2 | trn acc:65.6, f1:51.8, auc:68.6 | loss:0.652
[63,420] | tst acc:60.9, f1:24.5, auc:66.5 | trn acc:65.5, f1:52.1, auc:68.6 | loss:0.646
[65,92] | tst acc:62.0, f1:25.6, auc:65.9 | trn acc:65.6, f1:51.6, auc:68.7 | loss:0.648
[66,440] | tst acc:59.2, f1:21.8, auc:65.0 | trn acc:65.5, f1:51.7, auc:68.6 | loss:0.648
[68,112] | tst acc:65.3, f1:30.0, auc:70.3 | trn acc:65.8, f1:52.2, auc:69.0 | loss:0.640
[69,460] | tst acc:65.3, f1:28.1, auc:69.6 | trn acc:65.4, f1:51.4, auc:68.7 | loss:0.647
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:63.1, f1:26.8, auc:69.5 | trn acc:65.7, f1:52.3, auc:68.8 | loss:0.644
Average epoch runtime: 4.02 seconds
Total training time: 289.20 GPU seconds
SEED: 58475
Loading drug dataset...
Hyperparameters:
Namespace(seed=58475, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.0001, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=False, alpha=0.6, gamma=2.0, adam=True, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING ADAM OPTIMIZER
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
[0,0] | tst acc:48.3, f1:35.4, auc:49.6 | trn acc:52.6, f1:45.1, auc:53.1 | loss:4.060
[1,348] | tst acc:54.8, f1:40.9, auc:53.7 | trn acc:52.3, f1:40.6, auc:53.1 | loss:4.150
[3,20] | tst acc:51.2, f1:34.3, auc:49.5 | trn acc:52.4, f1:40.8, auc:53.1 | loss:4.121
[4,368] | tst acc:54.6, f1:40.0, auc:55.0 | trn acc:52.6, f1:40.9, auc:53.4 | loss:4.082
[6,40] | tst acc:53.4, f1:39.4, auc:52.3 | trn acc:52.7, f1:41.1, auc:53.5 | loss:4.027
[7,388] | tst acc:53.6, f1:39.9, auc:54.3 | trn acc:53.1, f1:41.4, auc:53.8 | loss:3.915
[9,60] | tst acc:50.7, f1:36.9, auc:51.3 | trn acc:53.3, f1:41.3, auc:54.1 | loss:3.788
[10,408] | tst acc:52.2, f1:38.0, auc:54.2 | trn acc:53.7, f1:41.5, auc:54.5 | loss:3.633
[12,80] | tst acc:54.1, f1:40.9, auc:56.2 | trn acc:54.1, f1:42.2, auc:55.0 | loss:3.465
[13,428] | tst acc:54.3, f1:39.1, auc:54.9 | trn acc:54.6, f1:42.0, auc:55.4 | loss:3.300
[15,100] | tst acc:57.5, f1:46.8, auc:58.8 | trn acc:55.1, f1:42.6, auc:56.0 | loss:3.139
[16,448] | tst acc:56.7, f1:42.3, auc:59.2 | trn acc:55.6, f1:42.7, auc:56.6 | loss:2.996
[18,120] | tst acc:55.5, f1:39.7, auc:56.3 | trn acc:56.4, f1:43.1, auc:57.2 | loss:2.863
[19,468] | tst acc:55.5, f1:41.6, auc:57.7 | trn acc:56.9, f1:43.6, auc:57.9 | loss:2.732
[21,140] | tst acc:59.4, f1:42.3, auc:57.7 | trn acc:57.6, f1:43.8, auc:58.6 | loss:2.585
[22,488] | tst acc:60.1, f1:45.8, auc:63.6 | trn acc:58.2, f1:44.5, auc:59.4 | loss:2.478
[24,160] | tst acc:59.4, f1:45.0, auc:62.0 | trn acc:58.9, f1:44.8, auc:60.2 | loss:2.362
[25,508] | tst acc:63.0, f1:46.9, auc:62.2 | trn acc:59.6, f1:45.3, auc:61.0 | loss:2.259
[27,180] | tst acc:61.1, f1:46.4, auc:64.2 | trn acc:60.3, f1:45.7, auc:61.8 | loss:2.154
[28,528] | tst acc:62.7, f1:46.4, auc:64.1 | trn acc:60.9, f1:46.6, auc:62.5 | loss:2.071
[30,200] | tst acc:62.5, f1:45.1, auc:63.0 | trn acc:61.6, f1:46.9, auc:63.4 | loss:1.965
[31,548] | tst acc:64.2, f1:45.8, auc:66.5 | trn acc:62.2, f1:47.5, auc:64.2 | loss:1.855
[33,220] | tst acc:61.1, f1:43.0, auc:65.7 | trn acc:62.8, f1:48.1, auc:64.5 | loss:1.482
[34,568] | tst acc:64.4, f1:49.3, auc:68.8 | trn acc:63.0, f1:48.0, auc:64.4 | loss:0.711
[36,240] | tst acc:61.8, f1:45.0, auc:63.9 | trn acc:63.2, f1:48.1, auc:64.8 | loss:0.674
[37,588] | tst acc:65.6, f1:50.2, auc:69.4 | trn acc:63.6, f1:48.6, auc:65.3 | loss:0.663
[39,260] | tst acc:65.9, f1:50.0, auc:69.0 | trn acc:63.7, f1:48.5, auc:65.4 | loss:0.668
[40,608] | tst acc:63.2, f1:47.8, auc:64.9 | trn acc:63.8, f1:48.6, auc:65.6 | loss:0.662
[42,280] | tst acc:65.4, f1:47.1, auc:68.0 | trn acc:64.3, f1:48.7, auc:66.1 | loss:0.662
[43,628] | tst acc:64.4, f1:47.9, auc:67.5 | trn acc:64.2, f1:48.9, auc:66.1 | loss:0.660
[45,300] | tst acc:67.3, f1:51.4, auc:70.8 | trn acc:64.5, f1:49.0, auc:66.5 | loss:0.656
[46,648] | tst acc:62.3, f1:42.5, auc:63.2 | trn acc:64.6, f1:49.2, auc:66.4 | loss:0.659
[48,320] | tst acc:66.6, f1:50.9, auc:70.4 | trn acc:64.7, f1:49.1, auc:66.6 | loss:0.657
[49,668] | tst acc:67.3, f1:49.3, auc:68.9 | trn acc:64.9, f1:49.5, auc:67.1 | loss:0.651
[51,340] | tst acc:63.9, f1:49.0, auc:68.7 | trn acc:65.3, f1:49.5, auc:67.3 | loss:0.649
[53,12] | tst acc:65.4, f1:46.7, auc:68.1 | trn acc:65.3, f1:49.9, auc:67.4 | loss:0.652
[54,360] | tst acc:68.0, f1:49.4, auc:67.8 | trn acc:65.5, f1:49.8, auc:67.5 | loss:0.651
[56,32] | tst acc:68.0, f1:52.3, auc:69.1 | trn acc:65.4, f1:49.9, auc:67.5 | loss:0.648
[57,380] | tst acc:65.4, f1:51.4, auc:70.3 | trn acc:65.8, f1:50.1, auc:67.8 | loss:0.645
[59,52] | tst acc:62.7, f1:42.8, auc:66.4 | trn acc:65.6, f1:50.0, auc:67.8 | loss:0.651
[60,400] | tst acc:67.1, f1:50.9, auc:69.4 | trn acc:66.0, f1:50.4, auc:68.2 | loss:0.644
[62,72] | tst acc:70.0, f1:56.1, auc:73.3 | trn acc:66.0, f1:50.2, auc:68.1 | loss:0.648
[63,420] | tst acc:64.2, f1:48.1, auc:70.1 | trn acc:66.0, f1:50.6, auc:68.2 | loss:0.640
[65,92] | tst acc:65.6, f1:48.4, auc:66.4 | trn acc:66.1, f1:50.3, auc:68.3 | loss:0.645
[66,440] | tst acc:66.1, f1:52.5, auc:73.8 | trn acc:65.8, f1:50.2, auc:67.9 | loss:0.647
[68,112] | tst acc:65.9, f1:50.0, auc:70.0 | trn acc:66.1, f1:50.4, auc:68.3 | loss:0.640
[69,460] | tst acc:66.8, f1:50.7, auc:70.7 | trn acc:65.9, f1:50.4, auc:68.0 | loss:0.644
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:61.3, f1:43.1, auc:63.6 | trn acc:65.9, f1:50.0, auc:68.2 | loss:0.645
Average epoch runtime: 4.00 seconds
Total training time: 288.34 GPU seconds
SEED: 94707
Loading drug dataset...
Hyperparameters:
Namespace(seed=94707, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.0001, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=False, alpha=0.6, gamma=2.0, adam=True, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING ADAM OPTIMIZER
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
[0,0] | tst acc:54.2, f1:55.2, auc:56.6 | trn acc:52.9, f1:45.3, auc:54.3 | loss:4.265
[1,348] | tst acc:53.4, f1:53.3, auc:51.8 | trn acc:51.4, f1:40.8, auc:52.8 | loss:4.377
[3,20] | tst acc:51.4, f1:50.0, auc:50.2 | trn acc:51.5, f1:40.8, auc:53.0 | loss:4.357
[4,368] | tst acc:49.4, f1:49.0, auc:52.2 | trn acc:51.5, f1:40.8, auc:53.1 | loss:4.303
[6,40] | tst acc:52.0, f1:53.6, auc:52.1 | trn acc:51.7, f1:41.0, auc:53.3 | loss:4.225
[7,388] | tst acc:53.1, f1:53.1, auc:52.7 | trn acc:51.8, f1:41.1, auc:53.4 | loss:4.161
[9,60] | tst acc:53.1, f1:55.4, auc:53.8 | trn acc:52.2, f1:41.5, auc:54.1 | loss:4.007
[10,408] | tst acc:52.8, f1:53.2, auc:55.4 | trn acc:52.3, f1:41.7, auc:54.4 | loss:3.851
[12,80] | tst acc:55.6, f1:55.8, auc:54.9 | trn acc:52.9, f1:42.1, auc:55.0 | loss:3.649
[13,428] | tst acc:52.8, f1:52.1, auc:52.7 | trn acc:53.6, f1:42.7, auc:55.7 | loss:3.458
[15,100] | tst acc:54.2, f1:54.0, auc:56.2 | trn acc:54.1, f1:43.1, auc:56.5 | loss:3.273
[16,448] | tst acc:54.8, f1:52.4, auc:57.6 | trn acc:54.5, f1:43.3, auc:57.0 | loss:3.122
[18,120] | tst acc:51.1, f1:49.0, auc:52.0 | trn acc:55.2, f1:43.8, auc:57.6 | loss:2.969
[19,468] | tst acc:54.2, f1:52.6, auc:55.1 | trn acc:55.8, f1:43.9, auc:58.4 | loss:2.820
[21,140] | tst acc:56.2, f1:55.1, auc:56.1 | trn acc:56.4, f1:44.6, auc:59.0 | loss:2.689
[22,488] | tst acc:55.1, f1:53.4, auc:58.1 | trn acc:57.2, f1:45.2, auc:59.8 | loss:2.557
[24,160] | tst acc:52.5, f1:50.9, auc:53.9 | trn acc:57.9, f1:45.5, auc:60.6 | loss:2.445
[25,508] | tst acc:55.6, f1:52.3, auc:57.0 | trn acc:58.6, f1:45.7, auc:61.3 | loss:2.337
[27,180] | tst acc:55.9, f1:51.9, auc:58.1 | trn acc:59.2, f1:46.1, auc:61.9 | loss:2.227
[28,528] | tst acc:55.9, f1:50.6, auc:60.3 | trn acc:60.0, f1:46.7, auc:62.8 | loss:2.120
[30,200] | tst acc:56.5, f1:53.3, auc:61.0 | trn acc:60.7, f1:47.3, auc:63.6 | loss:2.032
[31,548] | tst acc:54.8, f1:50.3, auc:58.1 | trn acc:61.5, f1:47.9, auc:64.3 | loss:1.932
[33,220] | tst acc:59.3, f1:54.4, auc:63.1 | trn acc:62.1, f1:48.3, auc:65.1 | loss:1.788
[34,568] | tst acc:54.0, f1:49.2, auc:59.1 | trn acc:62.7, f1:48.5, auc:64.6 | loss:1.048
[36,240] | tst acc:61.9, f1:59.0, auc:64.0 | trn acc:62.7, f1:48.4, auc:64.9 | loss:0.680
[37,588] | tst acc:59.9, f1:56.2, auc:62.1 | trn acc:63.1, f1:48.9, auc:65.4 | loss:0.673
[39,260] | tst acc:56.8, f1:51.7, auc:60.0 | trn acc:63.4, f1:49.0, auc:65.7 | loss:0.669
[40,608] | tst acc:59.9, f1:55.1, auc:61.8 | trn acc:63.5, f1:49.1, auc:65.9 | loss:0.664
[42,280] | tst acc:59.3, f1:53.2, auc:61.6 | trn acc:63.8, f1:49.3, auc:66.3 | loss:0.658
[43,628] | tst acc:59.3, f1:52.6, auc:62.2 | trn acc:64.0, f1:49.2, auc:66.2 | loss:0.666
[45,300] | tst acc:59.0, f1:54.0, auc:62.3 | trn acc:64.3, f1:49.5, auc:66.7 | loss:0.656
[46,648] | tst acc:58.2, f1:52.6, auc:63.6 | trn acc:64.3, f1:49.6, auc:66.8 | loss:0.662
[48,320] | tst acc:58.2, f1:53.5, auc:61.7 | trn acc:64.6, f1:49.8, auc:67.2 | loss:0.655
[49,668] | tst acc:57.9, f1:52.7, auc:61.5 | trn acc:64.9, f1:49.8, auc:67.4 | loss:0.655
[51,340] | tst acc:57.9, f1:52.7, auc:63.6 | trn acc:64.8, f1:50.0, auc:67.5 | loss:0.648
[53,12] | tst acc:57.6, f1:52.5, auc:63.8 | trn acc:65.3, f1:50.1, auc:67.9 | loss:0.653
[54,360] | tst acc:55.6, f1:49.5, auc:60.7 | trn acc:65.1, f1:50.1, auc:67.7 | loss:0.651
[56,32] | tst acc:57.3, f1:51.1, auc:63.0 | trn acc:65.3, f1:50.3, auc:68.0 | loss:0.646
[57,380] | tst acc:57.9, f1:52.1, auc:63.0 | trn acc:65.5, f1:50.7, auc:68.1 | loss:0.647
[59,52] | tst acc:60.7, f1:55.3, auc:65.4 | trn acc:65.7, f1:50.3, auc:68.5 | loss:0.642
[60,400] | tst acc:60.2, f1:53.5, auc:66.1 | trn acc:65.8, f1:50.7, auc:68.4 | loss:0.648
[62,72] | tst acc:58.5, f1:52.7, auc:64.6 | trn acc:65.7, f1:50.5, auc:68.5 | loss:0.641
[63,420] | tst acc:60.7, f1:54.1, auc:66.7 | trn acc:65.9, f1:50.2, auc:68.5 | loss:0.646
[65,92] | tst acc:59.0, f1:51.8, auc:61.9 | trn acc:65.8, f1:50.9, auc:68.6 | loss:0.641
[66,440] | tst acc:58.2, f1:53.2, auc:63.3 | trn acc:65.7, f1:50.6, auc:68.4 | loss:0.646
[68,112] | tst acc:59.9, f1:53.9, auc:63.1 | trn acc:65.9, f1:50.7, auc:68.5 | loss:0.641
[69,460] | tst acc:62.1, f1:56.8, auc:65.6 | trn acc:65.9, f1:50.7, auc:68.6 | loss:0.643
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:61.3, f1:55.7, auc:63.8 | trn acc:65.8, f1:50.4, auc:68.6 | loss:0.641
Average epoch runtime: 4.01 seconds
Total training time: 288.51 GPU seconds
LR = 0.001 using OneCycleSEED: 5497
Loading drug dataset...
Hyperparameters:
Namespace(seed=5497, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.001, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=False, alpha=0.6, gamma=2.0, adam=True, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING ADAM OPTIMIZER
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
[0,0] | tst acc:51.7, f1:21.4, auc:62.3 | trn acc:52.0, f1:41.1, auc:53.9 | loss:4.221
[1,348] | tst acc:48.3, f1:16.8, auc:51.8 | trn acc:51.7, f1:41.8, auc:53.6 | loss:4.058
[3,20] | tst acc:49.8, f1:20.1, auc:56.5 | trn acc:52.5, f1:43.1, auc:54.8 | loss:3.711
[4,368] | tst acc:50.0, f1:22.4, auc:59.3 | trn acc:53.9, f1:44.6, auc:56.6 | loss:3.194
[6,40] | tst acc:55.5, f1:23.3, auc:62.5 | trn acc:55.8, f1:45.1, auc:58.7 | loss:2.740
[7,388] | tst acc:55.5, f1:19.0, auc:56.9 | trn acc:57.9, f1:46.7, auc:61.1 | loss:2.343
[9,60] | tst acc:57.0, f1:24.5, auc:66.0 | trn acc:60.5, f1:48.2, auc:63.6 | loss:2.033
[10,408] | tst acc:57.9, f1:24.9, auc:64.5 | trn acc:62.3, f1:49.5, auc:63.8 | loss:1.259
[12,80] | tst acc:62.4, f1:24.6, auc:66.0 | trn acc:63.3, f1:50.1, auc:66.0 | loss:0.674
[13,428] | tst acc:60.3, f1:24.2, auc:67.2 | trn acc:64.5, f1:51.2, auc:67.4 | loss:0.657
[15,100] | tst acc:65.1, f1:28.6, auc:70.6 | trn acc:65.8, f1:52.0, auc:69.0 | loss:0.645
[16,448] | tst acc:66.6, f1:26.8, auc:69.1 | trn acc:67.0, f1:53.0, auc:70.3 | loss:0.632
[18,120] | tst acc:66.4, f1:28.7, auc:70.2 | trn acc:68.4, f1:54.6, auc:71.8 | loss:0.622
[19,468] | tst acc:62.9, f1:21.3, auc:66.4 | trn acc:69.4, f1:55.1, auc:72.8 | loss:0.610
[21,140] | tst acc:66.4, f1:27.4, auc:69.8 | trn acc:70.8, f1:56.8, auc:74.4 | loss:0.595
[22,488] | tst acc:67.9, f1:29.7, auc:72.5 | trn acc:72.3, f1:57.9, auc:75.6 | loss:0.589
[24,160] | tst acc:66.8, f1:30.9, auc:75.7 | trn acc:73.6, f1:59.4, auc:76.8 | loss:0.574
[25,508] | tst acc:73.1, f1:34.9, auc:78.5 | trn acc:74.5, f1:60.3, auc:77.8 | loss:0.556
[27,180] | tst acc:70.5, f1:32.2, auc:76.7 | trn acc:75.2, f1:60.7, auc:78.3 | loss:0.560
[28,528] | tst acc:71.6, f1:31.6, auc:76.4 | trn acc:76.2, f1:61.8, auc:79.3 | loss:0.542
[30,200] | tst acc:73.8, f1:34.8, auc:77.7 | trn acc:76.5, f1:62.0, auc:79.7 | loss:0.533
[31,548] | tst acc:73.8, f1:33.3, auc:78.9 | trn acc:77.1, f1:62.6, auc:80.4 | loss:0.521
[33,220] | tst acc:76.0, f1:40.2, auc:82.7 | trn acc:77.6, f1:63.6, auc:81.2 | loss:0.507
[34,568] | tst acc:75.1, f1:39.4, auc:81.5 | trn acc:77.9, f1:63.9, auc:81.5 | loss:0.503
[36,240] | tst acc:75.1, f1:38.0, auc:82.2 | trn acc:78.6, f1:64.3, auc:82.2 | loss:0.489
[37,588] | tst acc:74.7, f1:37.0, auc:81.0 | trn acc:78.6, f1:64.7, auc:82.2 | loss:0.496
[39,260] | tst acc:76.6, f1:39.5, auc:82.2 | trn acc:78.9, f1:64.9, auc:82.7 | loss:0.488
[40,608] | tst acc:76.4, f1:37.2, auc:81.6 | trn acc:78.9, f1:65.2, auc:82.7 | loss:0.484
[42,280] | tst acc:76.0, f1:37.5, auc:82.8 | trn acc:79.0, f1:65.3, auc:82.8 | loss:0.484
[43,628] | tst acc:76.2, f1:38.4, auc:81.4 | trn acc:79.2, f1:65.3, auc:82.9 | loss:0.481
[45,300] | tst acc:75.1, f1:38.0, auc:81.8 | trn acc:79.3, f1:65.6, auc:82.9 | loss:0.485
[46,648] | tst acc:76.4, f1:40.7, auc:81.1 | trn acc:79.4, f1:65.9, auc:83.3 | loss:0.475
[48,320] | tst acc:78.2, f1:41.2, auc:82.9 | trn acc:79.6, f1:65.9, auc:83.3 | loss:0.477
[49,668] | tst acc:75.1, f1:34.5, auc:78.3 | trn acc:79.4, f1:65.9, auc:83.3 | loss:0.474
[51,340] | tst acc:78.2, f1:40.5, auc:84.9 | trn acc:79.8, f1:66.4, auc:83.6 | loss:0.471
[53,12] | tst acc:75.5, f1:37.1, auc:80.9 | trn acc:79.4, f1:65.8, auc:83.2 | loss:0.477
[54,360] | tst acc:76.2, f1:36.3, auc:80.5 | trn acc:79.8, f1:66.2, auc:83.6 | loss:0.472
[56,32] | tst acc:77.1, f1:40.7, auc:81.0 | trn acc:79.8, f1:66.5, auc:83.5 | loss:0.470
[57,380] | tst acc:76.2, f1:38.4, auc:81.4 | trn acc:79.7, f1:66.3, auc:83.5 | loss:0.473
[59,52] | tst acc:76.0, f1:38.9, auc:81.8 | trn acc:79.7, f1:66.3, auc:83.7 | loss:0.470
[60,400] | tst acc:76.2, f1:41.7, auc:83.6 | trn acc:79.8, f1:66.2, auc:83.7 | loss:0.468
[62,72] | tst acc:75.1, f1:36.7, auc:81.4 | trn acc:79.9, f1:66.5, auc:83.7 | loss:0.470
[63,420] | tst acc:76.0, f1:40.2, auc:81.4 | trn acc:79.6, f1:66.3, auc:83.5 | loss:0.472
[65,92] | tst acc:77.7, f1:42.7, auc:81.8 | trn acc:80.0, f1:66.5, auc:83.7 | loss:0.469
[66,440] | tst acc:76.0, f1:38.9, auc:80.4 | trn acc:79.8, f1:66.2, auc:83.6 | loss:0.471
[68,112] | tst acc:75.3, f1:38.3, auc:80.8 | trn acc:80.0, f1:66.7, auc:84.0 | loss:0.463
[69,460] | tst acc:75.5, f1:38.5, auc:82.4 | trn acc:79.8, f1:66.2, auc:83.7 | loss:0.469
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:76.0, f1:38.2, auc:81.7 | trn acc:79.9, f1:66.7, auc:83.7 | loss:0.470
Average epoch runtime: 4.01 seconds
Total training time: 288.38 GPU seconds
SEED: 58475
Loading drug dataset...
Hyperparameters:
Namespace(seed=58475, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.001, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=False, alpha=0.6, gamma=2.0, adam=True, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING ADAM OPTIMIZER
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
[0,0] | tst acc:48.3, f1:35.4, auc:49.6 | trn acc:52.6, f1:45.1, auc:53.1 | loss:4.060
[1,348] | tst acc:54.1, f1:40.5, auc:54.5 | trn acc:52.6, f1:40.8, auc:53.4 | loss:4.071
[3,20] | tst acc:52.6, f1:37.5, auc:50.7 | trn acc:53.5, f1:41.5, auc:54.2 | loss:3.708
[4,368] | tst acc:56.5, f1:38.6, auc:58.4 | trn acc:55.1, f1:42.4, auc:56.0 | loss:3.164
[6,40] | tst acc:58.7, f1:42.7, auc:59.1 | trn acc:57.0, f1:43.6, auc:58.1 | loss:2.702
[7,388] | tst acc:63.2, f1:48.1, auc:65.3 | trn acc:59.3, f1:45.2, auc:60.7 | loss:2.301
[9,60] | tst acc:61.5, f1:44.4, auc:63.2 | trn acc:61.4, f1:46.8, auc:63.2 | loss:1.989
[10,408] | tst acc:63.2, f1:44.4, auc:65.8 | trn acc:63.0, f1:48.0, auc:63.4 | loss:1.047
[12,80] | tst acc:61.3, f1:44.7, auc:67.3 | trn acc:63.7, f1:48.6, auc:65.6 | loss:0.661
[13,428] | tst acc:69.0, f1:52.4, auc:70.5 | trn acc:64.8, f1:49.2, auc:66.9 | loss:0.654
[15,100] | tst acc:68.0, f1:53.0, auc:73.3 | trn acc:65.9, f1:50.3, auc:68.1 | loss:0.650
[16,448] | tst acc:68.0, f1:50.2, auc:75.2 | trn acc:67.0, f1:51.3, auc:69.6 | loss:0.631
[18,120] | tst acc:68.3, f1:50.0, auc:74.3 | trn acc:68.3, f1:52.4, auc:71.0 | loss:0.616
[19,468] | tst acc:69.7, f1:52.6, auc:73.9 | trn acc:69.8, f1:53.9, auc:72.6 | loss:0.605
[21,140] | tst acc:71.9, f1:53.8, auc:74.2 | trn acc:71.1, f1:54.9, auc:73.9 | loss:0.593
[22,488] | tst acc:73.3, f1:57.1, auc:77.9 | trn acc:72.2, f1:56.1, auc:75.2 | loss:0.579
[24,160] | tst acc:74.8, f1:58.5, auc:79.8 | trn acc:73.3, f1:57.2, auc:76.2 | loss:0.566
[25,508] | tst acc:73.3, f1:57.5, auc:79.0 | trn acc:74.4, f1:58.3, auc:77.2 | loss:0.561
[27,180] | tst acc:74.3, f1:58.4, auc:79.1 | trn acc:75.2, f1:58.9, auc:78.2 | loss:0.547
[28,528] | tst acc:76.2, f1:60.6, auc:80.1 | trn acc:75.7, f1:59.8, auc:78.6 | loss:0.544
[30,200] | tst acc:78.4, f1:62.2, auc:80.5 | trn acc:76.7, f1:60.8, auc:79.5 | loss:0.534
[31,548] | tst acc:77.4, f1:60.8, auc:80.8 | trn acc:77.2, f1:61.4, auc:80.1 | loss:0.527
[33,220] | tst acc:75.2, f1:56.2, auc:81.6 | trn acc:77.4, f1:61.8, auc:80.1 | loss:0.528
[34,568] | tst acc:76.4, f1:57.4, auc:83.1 | trn acc:77.7, f1:61.9, auc:80.6 | loss:0.515
[36,240] | tst acc:76.7, f1:59.1, auc:80.8 | trn acc:78.1, f1:62.4, auc:81.1 | loss:0.512
[37,588] | tst acc:77.9, f1:61.0, auc:83.1 | trn acc:78.3, f1:62.7, auc:81.5 | loss:0.499
[39,260] | tst acc:76.2, f1:57.1, auc:82.1 | trn acc:78.5, f1:63.0, auc:81.7 | loss:0.499
[40,608] | tst acc:76.2, f1:57.5, auc:81.5 | trn acc:78.8, f1:63.4, auc:82.0 | loss:0.490
[42,280] | tst acc:78.1, f1:61.3, auc:82.9 | trn acc:79.3, f1:64.0, auc:82.5 | loss:0.486
[43,628] | tst acc:75.5, f1:56.0, auc:82.1 | trn acc:79.0, f1:63.6, auc:82.3 | loss:0.485
[45,300] | tst acc:78.4, f1:61.2, auc:83.9 | trn acc:79.5, f1:64.3, auc:82.9 | loss:0.477
[46,648] | tst acc:75.7, f1:56.7, auc:80.5 | trn acc:79.2, f1:64.0, auc:82.6 | loss:0.483
[48,320] | tst acc:79.3, f1:63.9, auc:83.8 | trn acc:79.4, f1:64.2, auc:82.9 | loss:0.478
[49,668] | tst acc:78.4, f1:60.9, auc:82.2 | trn acc:79.7, f1:64.9, auc:83.2 | loss:0.474
[51,340] | tst acc:79.3, f1:63.9, auc:84.5 | trn acc:79.9, f1:64.8, auc:83.3 | loss:0.470
[53,12] | tst acc:78.8, f1:63.3, auc:82.8 | trn acc:79.7, f1:65.0, auc:83.2 | loss:0.475
[54,360] | tst acc:81.5, f1:66.1, auc:84.6 | trn acc:79.8, f1:64.8, auc:83.4 | loss:0.471
[56,32] | tst acc:79.1, f1:63.9, auc:83.7 | trn acc:79.5, f1:64.6, auc:83.0 | loss:0.476
[57,380] | tst acc:79.1, f1:64.5, auc:85.4 | trn acc:79.9, f1:65.2, auc:83.4 | loss:0.469
[59,52] | tst acc:77.6, f1:60.1, auc:82.9 | trn acc:79.7, f1:64.8, auc:83.3 | loss:0.475
[60,400] | tst acc:78.8, f1:62.7, auc:83.7 | trn acc:80.0, f1:65.2, auc:83.5 | loss:0.468
[62,72] | tst acc:79.1, f1:63.3, auc:84.8 | trn acc:80.0, f1:65.1, auc:83.5 | loss:0.471
[63,420] | tst acc:80.8, f1:65.8, auc:85.4 | trn acc:79.9, f1:65.4, auc:83.6 | loss:0.467
[65,92] | tst acc:79.3, f1:63.6, auc:82.5 | trn acc:79.9, f1:65.0, auc:83.5 | loss:0.469
[66,440] | tst acc:80.0, f1:65.0, auc:86.8 | trn acc:79.7, f1:64.8, auc:83.2 | loss:0.474
[68,112] | tst acc:78.1, f1:61.9, auc:83.3 | trn acc:80.1, f1:65.4, auc:83.7 | loss:0.464
[69,460] | tst acc:79.6, f1:63.2, auc:84.8 | trn acc:79.7, f1:64.9, auc:83.4 | loss:0.471
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:77.4, f1:60.2, auc:81.7 | trn acc:80.1, f1:65.3, auc:83.7 | loss:0.466
Average epoch runtime: 4.01 seconds
Total training time: 288.38 GPU seconds
SEED: 94707
Loading drug dataset...
Hyperparameters:
Namespace(seed=94707, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='onecycle', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.001, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=False, alpha=0.6, gamma=2.0, adam=True, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING ADAM OPTIMIZER
Training...
INITIALIZING ONE CYCLE
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
[0,0] | tst acc:54.2, f1:55.2, auc:56.6 | trn acc:52.9, f1:45.3, auc:54.3 | loss:4.265
[1,348] | tst acc:53.4, f1:53.3, auc:52.1 | trn acc:51.6, f1:40.9, auc:53.1 | loss:4.292
[3,20] | tst acc:52.5, f1:50.9, auc:51.9 | trn acc:52.5, f1:41.8, auc:54.3 | loss:3.916
[4,368] | tst acc:55.1, f1:55.2, auc:55.5 | trn acc:53.9, f1:42.9, auc:56.3 | loss:3.308
[6,40] | tst acc:56.5, f1:56.0, auc:57.3 | trn acc:56.0, f1:44.1, auc:58.5 | loss:2.790
[7,388] | tst acc:58.8, f1:54.4, auc:61.0 | trn acc:58.2, f1:45.6, auc:60.9 | loss:2.384
[9,60] | tst acc:55.4, f1:49.4, auc:60.1 | trn acc:60.8, f1:47.4, auc:63.6 | loss:2.052
[10,408] | tst acc:59.9, f1:55.3, auc:62.8 | trn acc:62.6, f1:48.6, auc:63.8 | loss:1.298
[12,80] | tst acc:59.6, f1:54.0, auc:63.7 | trn acc:63.7, f1:49.0, auc:65.8 | loss:0.664
[13,428] | tst acc:60.5, f1:54.8, auc:63.1 | trn acc:64.9, f1:49.9, auc:67.3 | loss:0.655
[15,100] | tst acc:61.0, f1:55.2, auc:65.2 | trn acc:66.2, f1:51.0, auc:68.9 | loss:0.640
[16,448] | tst acc:59.9, f1:52.7, auc:66.8 | trn acc:67.3, f1:51.7, auc:70.1 | loss:0.631
[18,120] | tst acc:62.4, f1:54.9, auc:65.8 | trn acc:68.8, f1:52.9, auc:71.7 | loss:0.613
[19,468] | tst acc:59.3, f1:51.7, auc:65.8 | trn acc:70.2, f1:53.8, auc:73.0 | loss:0.604
[21,140] | tst acc:60.2, f1:50.9, auc:67.6 | trn acc:71.3, f1:54.9, auc:74.1 | loss:0.590
[22,488] | tst acc:58.8, f1:47.5, auc:66.9 | trn acc:72.7, f1:56.4, auc:75.3 | loss:0.579
[24,160] | tst acc:61.0, f1:49.3, auc:66.2 | trn acc:74.0, f1:57.6, auc:76.7 | loss:0.561
[25,508] | tst acc:62.4, f1:51.6, auc:70.1 | trn acc:75.0, f1:58.6, auc:77.5 | loss:0.557
[27,180] | tst acc:61.0, f1:50.4, auc:69.4 | trn acc:75.7, f1:59.4, auc:78.3 | loss:0.546
[28,528] | tst acc:63.8, f1:53.3, auc:71.4 | trn acc:76.4, f1:60.2, auc:79.0 | loss:0.536
[30,200] | tst acc:63.6, f1:52.7, auc:72.1 | trn acc:77.1, f1:61.3, auc:79.6 | loss:0.529
[31,548] | tst acc:62.4, f1:50.6, auc:68.6 | trn acc:77.3, f1:61.6, auc:79.8 | loss:0.531
[33,220] | tst acc:64.7, f1:52.5, auc:73.8 | trn acc:77.7, f1:62.1, auc:80.4 | loss:0.522
[34,568] | tst acc:61.9, f1:49.1, auc:70.5 | trn acc:78.1, f1:62.5, auc:80.9 | loss:0.506
[36,240] | tst acc:65.3, f1:55.3, auc:72.7 | trn acc:78.4, f1:62.8, auc:81.4 | loss:0.497
[37,588] | tst acc:63.0, f1:52.7, auc:70.9 | trn acc:78.5, f1:63.2, auc:81.6 | loss:0.497
[39,260] | tst acc:61.9, f1:49.8, auc:71.0 | trn acc:78.8, f1:63.5, auc:82.0 | loss:0.488
[40,608] | tst acc:63.0, f1:52.0, auc:73.0 | trn acc:79.0, f1:63.8, auc:82.2 | loss:0.485
[42,280] | tst acc:64.4, f1:52.6, auc:72.1 | trn acc:79.5, f1:64.5, auc:82.8 | loss:0.476
[43,628] | tst acc:62.1, f1:48.9, auc:73.2 | trn acc:79.3, f1:64.1, auc:82.5 | loss:0.483
[45,300] | tst acc:62.1, f1:50.7, auc:73.7 | trn acc:79.6, f1:64.6, auc:82.9 | loss:0.473
[46,648] | tst acc:62.7, f1:48.8, auc:73.3 | trn acc:79.6, f1:64.7, auc:82.8 | loss:0.479
[48,320] | tst acc:63.0, f1:50.9, auc:73.4 | trn acc:79.7, f1:64.7, auc:83.1 | loss:0.472
[49,668] | tst acc:63.3, f1:51.1, auc:73.6 | trn acc:79.9, f1:65.0, auc:83.1 | loss:0.472
[51,340] | tst acc:63.8, f1:53.3, auc:73.0 | trn acc:79.7, f1:64.8, auc:83.1 | loss:0.471
[53,12] | tst acc:65.0, f1:54.7, auc:75.2 | trn acc:80.0, f1:65.1, auc:83.4 | loss:0.470
[54,360] | tst acc:61.9, f1:49.8, auc:71.7 | trn acc:79.8, f1:64.9, auc:83.2 | loss:0.471
[56,32] | tst acc:64.4, f1:53.3, auc:74.0 | trn acc:79.9, f1:65.0, auc:83.3 | loss:0.468
[57,380] | tst acc:65.3, f1:54.6, auc:74.9 | trn acc:79.8, f1:65.1, auc:83.2 | loss:0.473
[59,52] | tst acc:64.1, f1:52.8, auc:74.7 | trn acc:80.2, f1:65.3, auc:83.7 | loss:0.461
[60,400] | tst acc:62.4, f1:49.4, auc:74.3 | trn acc:80.1, f1:65.3, auc:83.4 | loss:0.467
[62,72] | tst acc:62.7, f1:50.7, auc:74.8 | trn acc:80.0, f1:65.3, auc:83.6 | loss:0.464
[63,420] | tst acc:63.0, f1:51.3, auc:75.8 | trn acc:80.2, f1:65.1, auc:83.4 | loss:0.466
[65,92] | tst acc:63.0, f1:51.3, auc:74.0 | trn acc:80.0, f1:65.5, auc:83.6 | loss:0.465
[66,440] | tst acc:62.7, f1:51.1, auc:73.1 | trn acc:79.9, f1:65.3, auc:83.5 | loss:0.470
[68,112] | tst acc:62.7, f1:50.4, auc:75.5 | trn acc:80.0, f1:65.2, auc:83.4 | loss:0.465
[69,460] | tst acc:63.8, f1:51.9, auc:75.0 | trn acc:80.0, f1:65.2, auc:83.4 | loss:0.467
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:64.7, f1:52.8, auc:74.0 | trn acc:80.2, f1:65.5, auc:83.8 | loss:0.460
Average epoch runtime: 4.01 seconds
Total training time: 288.45 GPU seconds
LR = 0.0001 using Cosine AnnealingSEED: 5497
Loading drug dataset...
Hyperparameters:
Namespace(seed=5497, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='cosine', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.0001, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=False, alpha=0.6, gamma=2.0, adam=True, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING ADAM OPTIMIZER
INITIALIZING COSINE SCHEDULER
Training...
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
[0,0] | tst acc:51.7, f1:21.4, auc:62.3 | trn acc:52.0, f1:41.1, auc:53.9 | loss:4.221
[1,348] | tst acc:47.4, f1:16.6, auc:52.3 | trn acc:51.9, f1:41.9, auc:53.9 | loss:3.977
[3,20] | tst acc:49.6, f1:19.5, auc:56.7 | trn acc:52.9, f1:43.4, auc:55.3 | loss:3.568
[4,368] | tst acc:50.0, f1:22.4, auc:59.3 | trn acc:53.9, f1:44.6, auc:56.7 | loss:3.203
[6,40] | tst acc:53.5, f1:22.5, auc:61.4 | trn acc:54.9, f1:44.7, auc:57.9 | loss:2.942
[7,388] | tst acc:53.5, f1:19.0, auc:55.0 | trn acc:55.8, f1:45.6, auc:58.9 | loss:2.716
[9,60] | tst acc:51.1, f1:20.6, auc:60.8 | trn acc:57.0, f1:45.9, auc:59.9 | loss:2.524
[10,408] | tst acc:53.7, f1:23.2, auc:60.6 | trn acc:58.0, f1:46.7, auc:61.0 | loss:2.361
[12,80] | tst acc:56.1, f1:19.9, auc:62.0 | trn acc:58.8, f1:47.2, auc:62.0 | loss:2.234
[13,428] | tst acc:56.8, f1:22.7, auc:66.9 | trn acc:59.6, f1:47.8, auc:62.8 | loss:2.119
[15,100] | tst acc:60.3, f1:26.6, auc:66.4 | trn acc:60.6, f1:48.2, auc:63.8 | loss:2.015
[16,448] | tst acc:61.4, f1:22.7, auc:62.8 | trn acc:61.4, f1:48.7, auc:64.7 | loss:1.910
[18,120] | tst acc:60.7, f1:23.7, auc:65.3 | trn acc:62.3, f1:49.7, auc:65.4 | loss:1.781
[19,468] | tst acc:55.7, f1:17.8, auc:57.6 | trn acc:62.2, f1:49.2, auc:64.5 | loss:1.161
[21,140] | tst acc:57.9, f1:21.2, auc:62.4 | trn acc:62.5, f1:49.8, auc:65.1 | loss:0.683
[22,488] | tst acc:58.7, f1:24.1, auc:65.6 | trn acc:62.9, f1:49.7, auc:65.5 | loss:0.678
[24,160] | tst acc:60.5, f1:29.0, auc:69.9 | trn acc:63.2, f1:50.1, auc:65.9 | loss:0.672
[25,508] | tst acc:60.7, f1:25.6, auc:69.0 | trn acc:63.5, f1:50.4, auc:66.4 | loss:0.658
[27,180] | tst acc:60.5, f1:26.7, auc:68.3 | trn acc:63.8, f1:50.3, auc:66.6 | loss:0.669
[28,528] | tst acc:59.0, f1:21.7, auc:63.8 | trn acc:64.4, f1:50.9, auc:67.3 | loss:0.660
[30,200] | tst acc:62.4, f1:23.9, auc:66.0 | trn acc:64.5, f1:50.9, auc:67.4 | loss:0.656
[31,548] | tst acc:61.8, f1:23.6, auc:66.0 | trn acc:64.9, f1:51.2, auc:67.9 | loss:0.655
[33,220] | tst acc:65.1, f1:29.8, auc:71.9 | trn acc:65.3, f1:51.8, auc:68.4 | loss:0.649
[34,568] | tst acc:64.2, f1:30.5, auc:75.0 | trn acc:65.6, f1:52.1, auc:68.7 | loss:0.647
[36,240] | tst acc:61.8, f1:27.4, auc:70.3 | trn acc:66.1, f1:52.0, auc:69.3 | loss:0.638
[37,588] | tst acc:61.4, f1:24.0, auc:69.1 | trn acc:66.3, f1:52.6, auc:69.4 | loss:0.645
[39,260] | tst acc:66.4, f1:30.6, auc:72.9 | trn acc:66.7, f1:52.7, auc:70.0 | loss:0.639
[40,608] | tst acc:64.8, f1:29.1, auc:72.5 | trn acc:66.9, f1:53.2, auc:70.2 | loss:0.631
[42,280] | tst acc:68.8, f1:28.9, auc:72.7 | trn acc:67.1, f1:53.2, auc:70.4 | loss:0.632
[43,628] | tst acc:66.4, f1:29.4, auc:70.8 | trn acc:67.6, f1:53.5, auc:71.0 | loss:0.626
[45,300] | tst acc:64.6, f1:26.4, auc:71.4 | trn acc:68.1, f1:54.0, auc:71.3 | loss:0.629
[46,648] | tst acc:65.7, f1:30.2, auc:71.9 | trn acc:68.3, f1:54.4, auc:71.9 | loss:0.617
[48,320] | tst acc:62.7, f1:25.3, auc:71.5 | trn acc:68.8, f1:54.4, auc:72.1 | loss:0.621
[49,668] | tst acc:67.2, f1:26.5, auc:68.3 | trn acc:69.2, f1:55.1, auc:72.6 | loss:0.608
[51,340] | tst acc:69.9, f1:33.0, auc:77.1 | trn acc:69.5, f1:55.3, auc:73.0 | loss:0.607
[53,12] | tst acc:67.2, f1:29.2, auc:73.4 | trn acc:69.7, f1:55.5, auc:73.2 | loss:0.609
[54,360] | tst acc:64.4, f1:24.9, auc:68.8 | trn acc:70.3, f1:55.8, auc:73.8 | loss:0.603
[56,32] | tst acc:69.2, f1:31.2, auc:73.6 | trn acc:70.6, f1:56.4, auc:74.0 | loss:0.596
[57,380] | tst acc:68.1, f1:31.8, auc:75.4 | trn acc:70.8, f1:56.5, auc:74.3 | loss:0.596
[59,52] | tst acc:67.0, f1:30.4, auc:74.4 | trn acc:71.2, f1:57.0, auc:74.7 | loss:0.591
[60,400] | tst acc:68.8, f1:32.9, auc:77.5 | trn acc:71.6, f1:57.1, auc:75.1 | loss:0.585
[62,72] | tst acc:67.0, f1:30.4, auc:73.3 | trn acc:71.9, f1:57.5, auc:75.3 | loss:0.589
[63,420] | tst acc:66.2, f1:28.6, auc:74.1 | trn acc:72.1, f1:58.0, auc:75.5 | loss:0.582
[65,92] | tst acc:68.3, f1:30.6, auc:74.5 | trn acc:72.6, f1:58.0, auc:75.9 | loss:0.580
[66,440] | tst acc:67.2, f1:29.9, auc:72.5 | trn acc:72.8, f1:58.3, auc:76.1 | loss:0.578
[68,112] | tst acc:71.0, f1:34.5, auc:75.5 | trn acc:73.3, f1:59.0, auc:76.7 | loss:0.566
[69,460] | tst acc:72.1, f1:35.4, auc:76.9 | trn acc:73.3, f1:58.7, auc:76.7 | loss:0.570
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:69.9, f1:30.3, auc:76.3 | trn acc:73.7, f1:59.6, auc:77.0 | loss:0.567
Average epoch runtime: 4.01 seconds
Total training time: 288.58 GPU seconds
SEED: 58475
Loading drug dataset...
Hyperparameters:
Namespace(seed=58475, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='cosine', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.0001, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=False, alpha=0.6, gamma=2.0, adam=True, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING ADAM OPTIMIZER
INITIALIZING COSINE SCHEDULER
Training...
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
[0,0] | tst acc:48.3, f1:35.4, auc:49.6 | trn acc:52.6, f1:45.1, auc:53.1 | loss:4.060
[1,348] | tst acc:53.8, f1:41.1, auc:55.1 | trn acc:52.8, f1:40.9, auc:53.6 | loss:3.991
[3,20] | tst acc:52.6, f1:37.9, auc:51.0 | trn acc:53.9, f1:41.7, auc:54.6 | loss:3.564
[4,368] | tst acc:56.7, f1:38.8, auc:57.8 | trn acc:55.1, f1:42.4, auc:56.0 | loss:3.179
[6,40] | tst acc:58.4, f1:43.3, auc:57.5 | trn acc:56.1, f1:43.0, auc:57.0 | loss:2.913
[7,388] | tst acc:59.1, f1:45.5, auc:62.3 | trn acc:57.3, f1:43.9, auc:58.3 | loss:2.671
[9,60] | tst acc:57.2, f1:42.2, auc:58.9 | trn acc:58.2, f1:44.3, auc:59.6 | loss:2.477
[10,408] | tst acc:59.6, f1:42.5, auc:62.7 | trn acc:59.4, f1:45.1, auc:60.6 | loss:2.325
[12,80] | tst acc:58.2, f1:42.4, auc:64.5 | trn acc:60.1, f1:45.9, auc:61.5 | loss:2.202
[13,428] | tst acc:64.7, f1:50.2, auc:65.9 | trn acc:60.8, f1:46.2, auc:62.4 | loss:2.090
[15,100] | tst acc:63.7, f1:50.2, auc:68.9 | trn acc:61.7, f1:47.1, auc:63.3 | loss:1.990
[16,448] | tst acc:65.1, f1:49.5, auc:70.1 | trn acc:62.2, f1:47.5, auc:64.1 | loss:1.873
[18,120] | tst acc:63.0, f1:45.8, auc:66.0 | trn acc:62.8, f1:47.8, auc:64.6 | loss:1.641
[19,468] | tst acc:60.1, f1:42.8, auc:62.9 | trn acc:63.0, f1:48.3, auc:64.4 | loss:0.798
[21,140] | tst acc:64.9, f1:46.7, auc:64.9 | trn acc:63.4, f1:48.1, auc:65.0 | loss:0.672
[22,488] | tst acc:66.1, f1:50.9, auc:68.9 | trn acc:63.5, f1:48.5, auc:65.4 | loss:0.667
[24,160] | tst acc:63.0, f1:46.5, auc:66.9 | trn acc:63.8, f1:48.4, auc:65.5 | loss:0.662
[25,508] | tst acc:66.8, f1:49.3, auc:68.4 | trn acc:64.1, f1:48.7, auc:65.9 | loss:0.663
[27,180] | tst acc:67.8, f1:52.1, auc:69.0 | trn acc:64.4, f1:48.7, auc:66.3 | loss:0.658
[28,528] | tst acc:65.1, f1:48.8, auc:68.8 | trn acc:64.6, f1:49.3, auc:66.5 | loss:0.658
[30,200] | tst acc:63.9, f1:44.0, auc:67.6 | trn acc:65.1, f1:49.5, auc:67.1 | loss:0.654
[31,548] | tst acc:66.1, f1:47.2, auc:68.8 | trn acc:65.3, f1:49.8, auc:67.5 | loss:0.650
[33,220] | tst acc:63.9, f1:43.6, auc:68.9 | trn acc:65.6, f1:50.1, auc:67.7 | loss:0.651
[34,568] | tst acc:68.0, f1:52.3, auc:73.7 | trn acc:66.0, f1:50.3, auc:68.1 | loss:0.640
[36,240] | tst acc:63.5, f1:46.5, auc:68.0 | trn acc:66.4, f1:50.7, auc:68.4 | loss:0.643
[37,588] | tst acc:70.2, f1:55.7, auc:73.7 | trn acc:66.7, f1:51.1, auc:69.0 | loss:0.633
[39,260] | tst acc:68.5, f1:51.3, auc:73.2 | trn acc:66.9, f1:51.2, auc:69.3 | loss:0.636
[40,608] | tst acc:66.8, f1:49.6, auc:68.8 | trn acc:67.3, f1:51.4, auc:69.6 | loss:0.630
[42,280] | tst acc:70.0, f1:52.5, auc:73.3 | trn acc:67.8, f1:51.8, auc:70.3 | loss:0.627
[43,628] | tst acc:67.3, f1:50.4, auc:72.1 | trn acc:67.9, f1:52.1, auc:70.3 | loss:0.624
[45,300] | tst acc:70.9, f1:54.0, auc:75.4 | trn acc:68.5, f1:52.4, auc:71.0 | loss:0.617
[46,648] | tst acc:68.3, f1:48.4, auc:68.6 | trn acc:68.6, f1:52.6, auc:71.0 | loss:0.620
[48,320] | tst acc:71.2, f1:55.6, auc:74.4 | trn acc:68.9, f1:52.7, auc:71.5 | loss:0.615
[49,668] | tst acc:71.9, f1:53.8, auc:74.3 | trn acc:69.4, f1:53.5, auc:72.2 | loss:0.607
[51,340] | tst acc:69.5, f1:53.1, auc:74.7 | trn acc:69.9, f1:53.6, auc:72.5 | loss:0.603
[53,12] | tst acc:68.8, f1:51.1, auc:73.0 | trn acc:70.1, f1:54.2, auc:72.8 | loss:0.603
[54,360] | tst acc:74.3, f1:56.7, auc:74.6 | trn acc:70.5, f1:54.3, auc:73.2 | loss:0.599
[56,32] | tst acc:71.6, f1:55.3, auc:75.5 | trn acc:70.7, f1:54.7, auc:73.3 | loss:0.596
[57,380] | tst acc:70.4, f1:53.9, auc:77.9 | trn acc:71.3, f1:55.2, auc:74.0 | loss:0.589
[59,52] | tst acc:68.5, f1:50.9, auc:73.2 | trn acc:71.4, f1:55.2, auc:74.1 | loss:0.592
[60,400] | tst acc:72.4, f1:55.6, auc:76.2 | trn acc:72.0, f1:55.9, auc:74.7 | loss:0.582
[62,72] | tst acc:72.4, f1:57.6, auc:78.3 | trn acc:72.3, f1:56.1, auc:75.0 | loss:0.583
[63,420] | tst acc:74.3, f1:58.7, auc:78.0 | trn acc:72.6, f1:56.7, auc:75.4 | loss:0.573
[65,92] | tst acc:70.9, f1:52.2, auc:74.0 | trn acc:72.8, f1:56.6, auc:75.7 | loss:0.574
[66,440] | tst acc:75.0, f1:61.2, auc:80.0 | trn acc:72.8, f1:56.7, auc:75.5 | loss:0.575
[68,112] | tst acc:72.8, f1:55.7, auc:76.8 | trn acc:73.5, f1:57.3, auc:76.3 | loss:0.564
[69,460] | tst acc:73.1, f1:55.6, auc:77.5 | trn acc:73.4, f1:57.3, auc:76.2 | loss:0.567
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:70.2, f1:52.7, auc:73.8 | trn acc:73.8, f1:57.4, auc:76.7 | loss:0.562
Average epoch runtime: 4.00 seconds
Total training time: 288.35 GPU seconds
SEED: 94707
Loading drug dataset...
Hyperparameters:
Namespace(seed=94707, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='cosine', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.0001, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=False, alpha=0.6, gamma=2.0, adam=True, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING ADAM OPTIMIZER
INITIALIZING COSINE SCHEDULER
Training...
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
[0,0] | tst acc:54.2, f1:55.2, auc:56.6 | trn acc:52.9, f1:45.3, auc:54.3 | loss:4.265
[1,348] | tst acc:52.8, f1:52.7, auc:52.3 | trn acc:51.8, f1:41.1, auc:53.4 | loss:4.207
[3,20] | tst acc:52.0, f1:50.0, auc:52.2 | trn acc:52.9, f1:42.1, auc:54.8 | loss:3.757
[4,368] | tst acc:54.2, f1:54.7, auc:55.2 | trn acc:53.9, f1:42.9, auc:56.3 | loss:3.325
[6,40] | tst acc:55.1, f1:55.0, auc:55.3 | trn acc:55.1, f1:43.5, auc:57.6 | loss:3.010
[7,388] | tst acc:59.0, f1:57.2, auc:58.1 | trn acc:56.1, f1:44.4, auc:58.6 | loss:2.779
[9,60] | tst acc:55.9, f1:54.4, auc:57.9 | trn acc:57.2, f1:45.0, auc:60.0 | loss:2.573
[10,408] | tst acc:59.0, f1:57.5, auc:60.2 | trn acc:58.1, f1:45.7, auc:60.8 | loss:2.423
[12,80] | tst acc:55.9, f1:53.6, auc:59.4 | trn acc:59.0, f1:46.2, auc:61.8 | loss:2.276
[13,428] | tst acc:57.6, f1:53.4, auc:58.8 | trn acc:60.0, f1:46.9, auc:62.7 | loss:2.160
[15,100] | tst acc:57.9, f1:53.6, auc:61.3 | trn acc:60.9, f1:47.6, auc:63.8 | loss:2.042
[16,448] | tst acc:57.3, f1:52.4, auc:62.4 | trn acc:61.4, f1:47.9, auc:64.4 | loss:1.952
[18,120] | tst acc:58.8, f1:54.1, auc:58.0 | trn acc:62.2, f1:48.5, auc:65.1 | loss:1.834
[19,468] | tst acc:57.9, f1:53.9, auc:60.7 | trn acc:62.8, f1:48.6, auc:65.3 | loss:1.368
[21,140] | tst acc:57.3, f1:53.8, auc:61.1 | trn acc:62.9, f1:48.7, auc:65.0 | loss:0.699
[22,488] | tst acc:58.2, f1:52.3, auc:61.8 | trn acc:63.3, f1:49.2, auc:65.5 | loss:0.671
[24,160] | tst acc:59.0, f1:54.3, auc:58.4 | trn acc:63.6, f1:49.2, auc:66.0 | loss:0.665
[25,508] | tst acc:59.3, f1:53.5, auc:61.2 | trn acc:63.8, f1:49.1, auc:66.1 | loss:0.666
[27,180] | tst acc:57.6, f1:51.6, auc:62.4 | trn acc:64.2, f1:49.4, auc:66.5 | loss:0.658
[28,528] | tst acc:59.3, f1:52.9, auc:65.0 | trn acc:64.5, f1:49.6, auc:66.9 | loss:0.655
[30,200] | tst acc:57.1, f1:51.6, auc:63.7 | trn acc:64.8, f1:49.9, auc:67.3 | loss:0.653
[31,548] | tst acc:56.8, f1:50.8, auc:60.8 | trn acc:65.1, f1:50.2, auc:67.6 | loss:0.651
[33,220] | tst acc:60.5, f1:52.7, auc:65.9 | trn acc:65.4, f1:50.3, auc:68.1 | loss:0.650
[34,568] | tst acc:58.8, f1:51.0, auc:62.4 | trn acc:65.9, f1:50.6, auc:68.6 | loss:0.642
[36,240] | tst acc:63.8, f1:58.7, auc:66.3 | trn acc:66.2, f1:50.8, auc:68.8 | loss:0.638
[37,588] | tst acc:61.0, f1:55.8, auc:64.8 | trn acc:66.5, f1:51.3, auc:69.3 | loss:0.637
[39,260] | tst acc:56.8, f1:50.8, auc:61.9 | trn acc:67.0, f1:51.6, auc:69.7 | loss:0.634
[40,608] | tst acc:60.5, f1:53.9, auc:64.0 | trn acc:67.2, f1:51.8, auc:70.1 | loss:0.628
[42,280] | tst acc:59.6, f1:49.8, auc:65.4 | trn acc:67.7, f1:52.2, auc:70.7 | loss:0.621
[43,628] | tst acc:60.5, f1:51.4, auc:65.6 | trn acc:67.9, f1:52.2, auc:70.6 | loss:0.627
[45,300] | tst acc:61.3, f1:55.4, auc:65.3 | trn acc:68.4, f1:52.7, auc:71.4 | loss:0.615
[46,648] | tst acc:60.2, f1:52.5, auc:65.9 | trn acc:68.6, f1:53.0, auc:71.5 | loss:0.620
[48,320] | tst acc:58.8, f1:51.0, auc:65.8 | trn acc:69.1, f1:53.3, auc:72.1 | loss:0.610
[49,668] | tst acc:60.5, f1:52.4, auc:65.2 | trn acc:69.6, f1:53.6, auc:72.4 | loss:0.609
[51,340] | tst acc:59.0, f1:50.5, auc:67.0 | trn acc:69.7, f1:53.9, auc:72.7 | loss:0.601
[53,12] | tst acc:59.9, f1:52.0, auc:67.4 | trn acc:70.4, f1:54.3, auc:73.2 | loss:0.603
[54,360] | tst acc:57.9, f1:48.8, auc:64.6 | trn acc:70.4, f1:54.4, auc:73.4 | loss:0.599
[56,32] | tst acc:61.6, f1:54.4, auc:67.0 | trn acc:70.8, f1:54.8, auc:73.8 | loss:0.591
[57,380] | tst acc:61.9, f1:53.0, auc:67.8 | trn acc:71.2, f1:55.4, auc:74.0 | loss:0.593
[59,52] | tst acc:62.4, f1:53.3, auc:68.8 | trn acc:71.8, f1:55.4, auc:74.8 | loss:0.581
[60,400] | tst acc:61.3, f1:51.6, auc:68.9 | trn acc:72.1, f1:56.0, auc:74.8 | loss:0.585
[62,72] | tst acc:61.6, f1:52.8, auc:69.4 | trn acc:72.3, f1:56.2, auc:75.3 | loss:0.576
[63,420] | tst acc:63.6, f1:53.1, auc:70.2 | trn acc:72.6, f1:56.0, auc:75.4 | loss:0.577
[65,92] | tst acc:61.0, f1:50.7, auc:66.4 | trn acc:73.0, f1:57.0, auc:75.8 | loss:0.570
[66,440] | tst acc:61.6, f1:52.8, auc:67.4 | trn acc:73.2, f1:57.2, auc:76.0 | loss:0.572
[68,112] | tst acc:62.1, f1:52.5, auc:69.6 | trn acc:73.6, f1:57.4, auc:76.2 | loss:0.565
[69,460] | tst acc:62.7, f1:53.8, auc:70.4 | trn acc:73.8, f1:57.6, auc:76.5 | loss:0.564
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:63.3, f1:55.2, auc:69.4 | trn acc:74.0, f1:57.7, auc:77.0 | loss:0.556
Average epoch runtime: 4.01 seconds
Total training time: 288.46 GPU seconds
LR = 0.001 using Cosine AnnealingSEED: 5497
Loading drug dataset...
Hyperparameters:
Namespace(seed=5497, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='cosine', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.001, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=False, alpha=0.6, gamma=2.0, adam=True, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING ADAM OPTIMIZER
INITIALIZING COSINE SCHEDULER
Training...
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
[0,0] | tst acc:52.0, f1:21.4, auc:62.4 | trn acc:52.0, f1:41.1, auc:53.9 | loss:4.221
[1,348] | tst acc:55.9, f1:21.7, auc:61.8 | trn acc:56.1, f1:45.1, auc:58.8 | loss:2.843
[3,20] | tst acc:61.4, f1:24.7, auc:67.9 | trn acc:63.2, f1:50.4, auc:63.8 | loss:1.140
[4,368] | tst acc:64.4, f1:31.2, auc:71.9 | trn acc:65.8, f1:52.5, auc:68.9 | loss:0.644
[6,40] | tst acc:66.2, f1:28.6, auc:71.6 | trn acc:68.7, f1:54.5, auc:72.2 | loss:0.619
[7,388] | tst acc:70.3, f1:30.6, auc:72.3 | trn acc:70.8, f1:56.7, auc:74.2 | loss:0.599
[9,60] | tst acc:70.7, f1:35.6, auc:77.0 | trn acc:72.6, f1:58.2, auc:76.1 | loss:0.579
[10,408] | tst acc:68.8, f1:29.6, auc:74.2 | trn acc:74.3, f1:60.0, auc:77.6 | loss:0.561
[12,80] | tst acc:72.9, f1:34.7, auc:75.7 | trn acc:75.4, f1:61.0, auc:78.6 | loss:0.554
[13,428] | tst acc:71.2, f1:31.2, auc:77.0 | trn acc:76.1, f1:61.9, auc:79.2 | loss:0.542
[15,100] | tst acc:75.5, f1:36.4, auc:79.3 | trn acc:76.9, f1:62.4, auc:80.1 | loss:0.524
[16,448] | tst acc:75.1, f1:35.2, auc:79.0 | trn acc:77.6, f1:63.2, auc:81.0 | loss:0.507
[18,120] | tst acc:74.2, f1:35.9, auc:81.3 | trn acc:78.0, f1:64.0, auc:81.7 | loss:0.501
[19,468] | tst acc:74.7, f1:34.1, auc:77.8 | trn acc:78.4, f1:64.2, auc:81.9 | loss:0.495
[21,140] | tst acc:74.0, f1:35.0, auc:79.0 | trn acc:78.7, f1:64.9, auc:82.3 | loss:0.488
[22,488] | tst acc:74.2, f1:36.6, auc:79.9 | trn acc:79.0, f1:64.9, auc:82.6 | loss:0.488
[24,160] | tst acc:75.3, f1:39.6, auc:82.6 | trn acc:79.3, f1:65.7, auc:83.0 | loss:0.481
[25,508] | tst acc:78.2, f1:43.8, auc:82.8 | trn acc:79.4, f1:65.9, auc:83.1 | loss:0.474
[27,180] | tst acc:74.7, f1:37.6, auc:80.6 | trn acc:79.4, f1:65.6, auc:82.9 | loss:0.482
[28,528] | tst acc:76.2, f1:39.1, auc:80.3 | trn acc:79.7, f1:66.2, auc:83.5 | loss:0.472
[30,200] | tst acc:77.5, f1:41.1, auc:81.8 | trn acc:79.7, f1:66.1, auc:83.4 | loss:0.473
[31,548] | tst acc:76.2, f1:38.4, auc:82.1 | trn acc:79.8, f1:66.1, auc:83.5 | loss:0.472
[33,220] | tst acc:77.3, f1:42.2, auc:84.0 | trn acc:79.9, f1:66.6, auc:83.7 | loss:0.469
[34,568] | tst acc:77.7, f1:43.3, auc:83.7 | trn acc:79.9, f1:66.5, auc:83.7 | loss:0.470
[36,240] | tst acc:77.3, f1:41.6, auc:83.5 | trn acc:80.2, f1:66.6, auc:84.0 | loss:0.461
[37,588] | tst acc:76.6, f1:38.9, auc:82.5 | trn acc:79.9, f1:66.6, auc:83.7 | loss:0.471
[39,260] | tst acc:77.9, f1:42.9, auc:83.0 | trn acc:80.2, f1:66.9, auc:84.1 | loss:0.464
[40,608] | tst acc:77.1, f1:39.3, auc:82.0 | trn acc:80.1, f1:66.9, auc:84.1 | loss:0.463
[42,280] | tst acc:77.9, f1:41.6, auc:83.5 | trn acc:80.2, f1:66.9, auc:84.0 | loss:0.465
[43,628] | tst acc:77.3, f1:40.2, auc:82.0 | trn acc:80.2, f1:66.9, auc:84.1 | loss:0.461
[45,300] | tst acc:76.9, f1:41.1, auc:82.6 | trn acc:80.3, f1:67.1, auc:84.0 | loss:0.465
[46,648] | tst acc:77.7, f1:42.0, auc:81.9 | trn acc:80.4, f1:67.3, auc:84.4 | loss:0.458
[48,320] | tst acc:77.7, f1:42.0, auc:83.0 | trn acc:80.6, f1:67.3, auc:84.4 | loss:0.458
[49,668] | tst acc:76.2, f1:38.4, auc:80.1 | trn acc:80.3, f1:67.1, auc:84.3 | loss:0.458
[51,340] | tst acc:79.7, f1:45.0, auc:85.1 | trn acc:80.6, f1:67.7, auc:84.6 | loss:0.453
[53,12] | tst acc:76.4, f1:40.0, auc:81.6 | trn acc:80.3, f1:67.0, auc:84.2 | loss:0.460
[54,360] | tst acc:77.5, f1:41.8, auc:81.7 | trn acc:80.7, f1:67.5, auc:84.6 | loss:0.455
[56,32] | tst acc:76.6, f1:40.2, auc:82.1 | trn acc:80.6, f1:67.6, auc:84.6 | loss:0.454
[57,380] | tst acc:77.3, f1:40.9, auc:82.6 | trn acc:80.5, f1:67.5, auc:84.5 | loss:0.457
[59,52] | tst acc:77.5, f1:41.1, auc:82.7 | trn acc:80.6, f1:67.5, auc:84.7 | loss:0.454
[60,400] | tst acc:76.0, f1:40.2, auc:84.1 | trn acc:80.7, f1:67.6, auc:84.7 | loss:0.451
[62,72] | tst acc:77.1, f1:40.0, auc:81.8 | trn acc:80.8, f1:67.8, auc:84.8 | loss:0.452
[63,420] | tst acc:77.1, f1:42.0, auc:82.5 | trn acc:80.5, f1:67.5, auc:84.6 | loss:0.455
[65,92] | tst acc:78.2, f1:43.2, auc:83.2 | trn acc:80.9, f1:67.7, auc:84.8 | loss:0.450
[66,440] | tst acc:77.5, f1:41.8, auc:82.4 | trn acc:80.7, f1:67.6, auc:84.7 | loss:0.453
[68,112] | tst acc:76.0, f1:38.9, auc:82.3 | trn acc:80.9, f1:68.1, auc:85.2 | loss:0.445
[69,460] | tst acc:77.1, f1:40.0, auc:83.4 | trn acc:80.9, f1:67.8, auc:85.0 | loss:0.448
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:76.0, f1:38.2, auc:82.7 | trn acc:80.8, f1:68.0, auc:84.9 | loss:0.450
Average epoch runtime: 4.01 seconds
Total training time: 288.55 GPU seconds
SEED: 58475
Loading drug dataset...
Hyperparameters:
Namespace(seed=58475, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='cosine', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.001, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=False, alpha=0.6, gamma=2.0, adam=True, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING ADAM OPTIMIZER
INITIALIZING COSINE SCHEDULER
Training...
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
[0,0] | tst acc:48.3, f1:35.4, auc:49.7 | trn acc:52.6, f1:45.1, auc:53.1 | loss:4.060
[1,348] | tst acc:64.2, f1:47.7, auc:67.9 | trn acc:57.5, f1:43.8, auc:58.4 | loss:2.806
[3,20] | tst acc:63.7, f1:46.3, auc:64.2 | trn acc:63.8, f1:48.6, auc:63.7 | loss:1.059
[4,368] | tst acc:69.2, f1:50.0, auc:74.0 | trn acc:66.5, f1:50.7, auc:68.6 | loss:0.640
[6,40] | tst acc:69.5, f1:51.7, auc:71.5 | trn acc:68.5, f1:52.5, auc:71.1 | loss:0.619
[7,388] | tst acc:71.9, f1:56.2, auc:76.2 | trn acc:70.8, f1:54.8, auc:73.7 | loss:0.594
[9,60] | tst acc:68.5, f1:48.6, auc:75.7 | trn acc:73.0, f1:57.0, auc:75.9 | loss:0.571
[10,408] | tst acc:73.8, f1:56.6, auc:76.2 | trn acc:74.6, f1:58.6, auc:77.5 | loss:0.557
[12,80] | tst acc:73.1, f1:54.8, auc:79.6 | trn acc:75.4, f1:59.7, auc:78.3 | loss:0.546
[13,428] | tst acc:77.2, f1:60.9, auc:81.0 | trn acc:76.3, f1:60.4, auc:79.2 | loss:0.535
[15,100] | tst acc:74.8, f1:56.4, auc:81.5 | trn acc:77.0, f1:61.2, auc:79.7 | loss:0.534
[16,448] | tst acc:78.4, f1:63.1, auc:82.9 | trn acc:77.4, f1:61.7, auc:80.4 | loss:0.521
[18,120] | tst acc:77.4, f1:60.2, auc:82.4 | trn acc:77.8, f1:62.2, auc:80.6 | loss:0.515
[19,468] | tst acc:74.0, f1:55.0, auc:79.7 | trn acc:78.2, f1:62.8, auc:81.2 | loss:0.512
[21,140] | tst acc:77.4, f1:60.5, auc:80.1 | trn acc:78.4, f1:62.8, auc:81.3 | loss:0.509
[22,488] | tst acc:77.2, f1:60.9, auc:82.3 | trn acc:78.5, f1:63.1, auc:81.7 | loss:0.500
[24,160] | tst acc:79.1, f1:62.7, auc:85.6 | trn acc:78.8, f1:63.4, auc:81.9 | loss:0.492
[25,508] | tst acc:76.9, f1:61.0, auc:83.2 | trn acc:79.1, f1:63.9, auc:82.4 | loss:0.488
[27,180] | tst acc:79.1, f1:63.0, auc:82.6 | trn acc:79.4, f1:64.2, auc:82.8 | loss:0.478
[28,528] | tst acc:79.3, f1:63.6, auc:83.8 | trn acc:79.4, f1:64.5, auc:82.9 | loss:0.480
[30,200] | tst acc:79.8, f1:63.8, auc:83.2 | trn acc:79.7, f1:64.7, auc:83.3 | loss:0.475
[31,548] | tst acc:78.8, f1:62.4, auc:84.0 | trn acc:79.9, f1:65.1, auc:83.6 | loss:0.470
[33,220] | tst acc:77.4, f1:58.0, auc:85.2 | trn acc:79.8, f1:65.1, auc:83.4 | loss:0.473
[34,568] | tst acc:77.6, f1:60.1, auc:85.2 | trn acc:80.0, f1:65.2, auc:83.6 | loss:0.466
[36,240] | tst acc:78.1, f1:60.6, auc:83.6 | trn acc:80.1, f1:65.4, auc:83.7 | loss:0.467
[37,588] | tst acc:79.6, f1:64.4, auc:84.6 | trn acc:80.1, f1:65.5, auc:83.9 | loss:0.462
[39,260] | tst acc:79.3, f1:63.6, auc:85.0 | trn acc:80.1, f1:65.5, auc:83.9 | loss:0.465
[40,608] | tst acc:77.2, f1:59.9, auc:83.3 | trn acc:80.3, f1:65.8, auc:84.0 | loss:0.461
[42,280] | tst acc:80.0, f1:64.4, auc:85.8 | trn acc:80.6, f1:66.2, auc:84.3 | loss:0.458
[43,628] | tst acc:78.1, f1:60.6, auc:84.3 | trn acc:80.1, f1:65.5, auc:84.0 | loss:0.460
[45,300] | tst acc:79.3, f1:62.9, auc:85.3 | trn acc:80.6, f1:66.2, auc:84.4 | loss:0.453
[46,648] | tst acc:78.1, f1:60.6, auc:83.9 | trn acc:80.2, f1:65.8, auc:84.1 | loss:0.461
[48,320] | tst acc:79.6, f1:63.2, auc:85.1 | trn acc:80.4, f1:65.9, auc:84.2 | loss:0.457
[49,668] | tst acc:81.2, f1:66.7, auc:84.4 | trn acc:80.6, f1:66.4, auc:84.5 | loss:0.454
[51,340] | tst acc:82.0, f1:68.4, auc:86.4 | trn acc:80.8, f1:66.3, auc:84.6 | loss:0.450
[53,12] | tst acc:80.8, f1:66.4, auc:84.9 | trn acc:80.6, f1:66.4, auc:84.5 | loss:0.455
[54,360] | tst acc:81.0, f1:65.2, auc:85.3 | trn acc:80.7, f1:66.4, auc:84.7 | loss:0.450
[56,32] | tst acc:80.0, f1:66.1, auc:85.7 | trn acc:80.4, f1:66.1, auc:84.3 | loss:0.456
[57,380] | tst acc:79.8, f1:66.1, auc:86.8 | trn acc:80.8, f1:66.7, auc:84.7 | loss:0.449
[59,52] | tst acc:79.1, f1:62.7, auc:84.4 | trn acc:80.7, f1:66.3, auc:84.6 | loss:0.453
[60,400] | tst acc:80.8, f1:66.1, auc:85.9 | trn acc:80.9, f1:66.7, auc:84.8 | loss:0.448
[62,72] | tst acc:81.0, f1:66.7, auc:86.2 | trn acc:80.9, f1:66.6, auc:84.8 | loss:0.448
[63,420] | tst acc:81.5, f1:67.5, auc:86.9 | trn acc:80.8, f1:66.8, auc:85.0 | loss:0.446
[65,92] | tst acc:79.6, f1:63.5, auc:85.0 | trn acc:80.8, f1:66.5, auc:84.9 | loss:0.446
[66,440] | tst acc:80.0, f1:65.6, auc:87.8 | trn acc:80.7, f1:66.5, auc:84.6 | loss:0.451
[68,112] | tst acc:80.5, f1:65.8, auc:85.4 | trn acc:81.0, f1:67.0, auc:85.2 | loss:0.441
[69,460] | tst acc:81.7, f1:68.3, auc:86.8 | trn acc:80.8, f1:66.8, auc:84.9 | loss:0.447
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:79.8, f1:64.7, auc:84.7 | trn acc:81.2, f1:67.2, auc:85.2 | loss:0.441
Average epoch runtime: 4.01 seconds
Total training time: 288.42 GPU seconds
SEED: 94707
Loading drug dataset...
Hyperparameters:
Namespace(seed=94707, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='cosine', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.001, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=False, alpha=0.6, gamma=2.0, adam=True, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING ADAM OPTIMIZER
INITIALIZING COSINE SCHEDULER
Training...
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
[0,0] | tst acc:54.2, f1:55.2, auc:56.7 | trn acc:52.9, f1:45.3, auc:54.3 | loss:4.265
[1,348] | tst acc:56.2, f1:51.7, auc:58.4 | trn acc:56.3, f1:44.3, auc:58.6 | loss:2.935
[3,20] | tst acc:61.9, f1:55.4, auc:65.3 | trn acc:63.5, f1:49.2, auc:63.9 | loss:1.168
[4,368] | tst acc:60.7, f1:54.1, auc:66.9 | trn acc:66.4, f1:51.1, auc:68.9 | loss:0.640
[6,40] | tst acc:61.6, f1:54.7, auc:65.7 | trn acc:68.9, f1:53.0, auc:71.9 | loss:0.612
[7,388] | tst acc:62.7, f1:54.8, auc:69.1 | trn acc:71.1, f1:55.1, auc:74.0 | loss:0.594
[9,60] | tst acc:63.3, f1:52.2, auc:70.2 | trn acc:73.3, f1:57.1, auc:76.2 | loss:0.568
[10,408] | tst acc:63.3, f1:53.2, auc:70.2 | trn acc:74.7, f1:58.6, auc:77.2 | loss:0.560
[12,80] | tst acc:61.9, f1:49.8, auc:69.6 | trn acc:75.8, f1:59.6, auc:78.4 | loss:0.543
[13,428] | tst acc:64.1, f1:53.8, auc:71.6 | trn acc:76.6, f1:60.9, auc:79.1 | loss:0.538
[15,100] | tst acc:64.7, f1:53.9, auc:71.1 | trn acc:77.3, f1:61.7, auc:80.2 | loss:0.519
[16,448] | tst acc:61.3, f1:49.1, auc:69.8 | trn acc:77.6, f1:61.9, auc:80.4 | loss:0.512
[18,120] | tst acc:60.7, f1:48.7, auc:71.4 | trn acc:78.3, f1:62.9, auc:81.5 | loss:0.496
[19,468] | tst acc:63.8, f1:53.6, auc:72.8 | trn acc:78.8, f1:63.5, auc:81.8 | loss:0.491
[21,140] | tst acc:62.1, f1:49.6, auc:73.2 | trn acc:78.9, f1:63.7, auc:81.9 | loss:0.487
[22,488] | tst acc:62.1, f1:49.6, auc:73.0 | trn acc:79.1, f1:64.1, auc:82.2 | loss:0.484
[24,160] | tst acc:64.1, f1:52.1, auc:71.3 | trn acc:79.6, f1:64.8, auc:82.8 | loss:0.475
[25,508] | tst acc:63.8, f1:53.3, auc:72.9 | trn acc:79.6, f1:64.6, auc:82.8 | loss:0.477
[27,180] | tst acc:63.0, f1:51.7, auc:71.9 | trn acc:79.7, f1:64.8, auc:83.0 | loss:0.473
[28,528] | tst acc:63.8, f1:52.2, auc:74.9 | trn acc:79.9, f1:65.1, auc:83.2 | loss:0.469
[30,200] | tst acc:63.0, f1:52.0, auc:73.7 | trn acc:80.1, f1:65.4, auc:83.5 | loss:0.465
[31,548] | tst acc:64.1, f1:51.7, auc:71.3 | trn acc:80.0, f1:65.4, auc:83.3 | loss:0.470
[33,220] | tst acc:65.0, f1:54.1, auc:74.8 | trn acc:80.2, f1:65.6, auc:83.5 | loss:0.468
[34,568] | tst acc:63.3, f1:50.0, auc:73.4 | trn acc:80.4, f1:65.8, auc:83.7 | loss:0.460
[36,240] | tst acc:65.0, f1:53.7, auc:74.7 | trn acc:80.4, f1:65.8, auc:83.9 | loss:0.459
[37,588] | tst acc:63.3, f1:51.9, auc:73.6 | trn acc:80.2, f1:65.7, auc:83.6 | loss:0.465
[39,260] | tst acc:62.4, f1:51.3, auc:73.9 | trn acc:80.4, f1:65.7, auc:83.8 | loss:0.460
[40,608] | tst acc:63.6, f1:51.7, auc:74.9 | trn acc:80.4, f1:66.0, auc:83.9 | loss:0.459
[42,280] | tst acc:62.4, f1:49.4, auc:73.3 | trn acc:80.7, f1:66.5, auc:84.3 | loss:0.453
[43,628] | tst acc:61.9, f1:49.1, auc:74.6 | trn acc:80.4, f1:65.9, auc:83.9 | loss:0.461
[45,300] | tst acc:63.6, f1:52.7, auc:74.2 | trn acc:80.7, f1:66.4, auc:84.3 | loss:0.452
[46,648] | tst acc:63.6, f1:50.6, auc:74.2 | trn acc:80.6, f1:66.2, auc:84.1 | loss:0.459
[48,320] | tst acc:65.0, f1:53.7, auc:75.6 | trn acc:80.7, f1:66.4, auc:84.4 | loss:0.452
[49,668] | tst acc:63.6, f1:51.7, auc:74.9 | trn acc:80.8, f1:66.3, auc:84.3 | loss:0.453
[51,340] | tst acc:64.7, f1:53.9, auc:74.9 | trn acc:80.6, f1:66.4, auc:84.2 | loss:0.453
[53,12] | tst acc:64.4, f1:53.7, auc:76.6 | trn acc:81.0, f1:66.7, auc:84.5 | loss:0.451
[54,360] | tst acc:62.1, f1:49.6, auc:74.0 | trn acc:80.7, f1:66.3, auc:84.4 | loss:0.453
[56,32] | tst acc:63.6, f1:52.0, auc:75.0 | trn acc:80.8, f1:66.4, auc:84.5 | loss:0.449
[57,380] | tst acc:64.7, f1:53.2, auc:77.2 | trn acc:80.7, f1:66.5, auc:84.4 | loss:0.454
[59,52] | tst acc:66.1, f1:56.2, auc:76.9 | trn acc:81.1, f1:66.7, auc:84.8 | loss:0.443
[60,400] | tst acc:63.0, f1:50.2, auc:75.0 | trn acc:81.0, f1:66.8, auc:84.6 | loss:0.448
[62,72] | tst acc:64.4, f1:54.0, auc:77.4 | trn acc:80.9, f1:66.8, auc:84.8 | loss:0.445
[63,420] | tst acc:65.0, f1:54.1, auc:76.8 | trn acc:81.2, f1:66.6, auc:84.6 | loss:0.445
[65,92] | tst acc:64.4, f1:54.0, auc:76.5 | trn acc:81.0, f1:67.0, auc:84.8 | loss:0.445
[66,440] | tst acc:62.7, f1:50.4, auc:75.5 | trn acc:81.0, f1:67.0, auc:84.7 | loss:0.449
[68,112] | tst acc:64.1, f1:52.4, auc:77.7 | trn acc:81.0, f1:66.7, auc:84.7 | loss:0.445
[69,460] | tst acc:65.3, f1:53.6, auc:77.0 | trn acc:81.0, f1:66.9, auc:84.8 | loss:0.444
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:66.4, f1:56.7, auc:76.6 | trn acc:81.3, f1:67.2, auc:85.2 | loss:0.438
Average epoch runtime: 4.01 seconds
Total training time: 288.60 GPU seconds
LR = 0.0001 using no SchedulerSEED: 5497
Loading drug dataset...
Hyperparameters:
Namespace(seed=5497, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='none', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.0001, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=False, alpha=0.6, gamma=2.0, adam=True, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING ADAM OPTIMIZER
Training...
USING NO LR SCHEDULING
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
[0,0] | tst acc:51.7, f1:21.4, auc:62.3 | trn acc:52.0, f1:41.1, auc:53.9 | loss:4.221
[1,348] | tst acc:47.4, f1:16.6, auc:52.3 | trn acc:51.9, f1:41.9, auc:53.9 | loss:3.977
[3,20] | tst acc:49.6, f1:19.5, auc:56.7 | trn acc:52.9, f1:43.4, auc:55.3 | loss:3.568
[4,368] | tst acc:50.0, f1:22.4, auc:59.3 | trn acc:53.9, f1:44.6, auc:56.7 | loss:3.203
[6,40] | tst acc:53.5, f1:22.5, auc:61.4 | trn acc:54.9, f1:44.7, auc:57.9 | loss:2.942
[7,388] | tst acc:53.5, f1:19.0, auc:55.0 | trn acc:55.8, f1:45.6, auc:58.9 | loss:2.716
[9,60] | tst acc:51.1, f1:20.6, auc:60.9 | trn acc:57.0, f1:45.9, auc:59.9 | loss:2.524
[10,408] | tst acc:53.7, f1:23.2, auc:60.6 | trn acc:58.0, f1:46.7, auc:61.0 | loss:2.360
[12,80] | tst acc:56.1, f1:19.9, auc:62.0 | trn acc:58.8, f1:47.2, auc:62.0 | loss:2.234
[13,428] | tst acc:56.8, f1:22.7, auc:66.9 | trn acc:59.6, f1:47.8, auc:62.8 | loss:2.118
[15,100] | tst acc:60.3, f1:26.6, auc:66.4 | trn acc:60.6, f1:48.2, auc:63.8 | loss:2.015
[16,448] | tst acc:61.4, f1:22.7, auc:62.8 | trn acc:61.4, f1:48.7, auc:64.7 | loss:1.909
[18,120] | tst acc:60.7, f1:23.7, auc:65.3 | trn acc:62.3, f1:49.7, auc:65.4 | loss:1.779
[19,468] | tst acc:55.7, f1:17.8, auc:57.7 | trn acc:62.2, f1:49.2, auc:64.4 | loss:1.150
[21,140] | tst acc:57.9, f1:21.2, auc:62.4 | trn acc:62.5, f1:49.8, auc:65.2 | loss:0.683
[22,488] | tst acc:58.7, f1:24.1, auc:65.6 | trn acc:62.9, f1:49.7, auc:65.6 | loss:0.678
[24,160] | tst acc:60.5, f1:29.0, auc:69.9 | trn acc:63.2, f1:50.1, auc:66.0 | loss:0.672
[25,508] | tst acc:60.7, f1:25.6, auc:69.0 | trn acc:63.6, f1:50.4, auc:66.5 | loss:0.658
[27,180] | tst acc:60.5, f1:26.7, auc:68.3 | trn acc:63.8, f1:50.4, auc:66.6 | loss:0.669
[28,528] | tst acc:59.2, f1:21.8, auc:63.8 | trn acc:64.4, f1:50.9, auc:67.3 | loss:0.660
[30,200] | tst acc:62.4, f1:23.9, auc:66.0 | trn acc:64.5, f1:50.9, auc:67.4 | loss:0.656
[31,548] | tst acc:61.8, f1:23.6, auc:66.0 | trn acc:64.9, f1:51.2, auc:67.9 | loss:0.655
[33,220] | tst acc:65.1, f1:29.8, auc:71.9 | trn acc:65.3, f1:51.8, auc:68.4 | loss:0.648
[34,568] | tst acc:64.4, f1:30.6, auc:75.0 | trn acc:65.6, f1:52.1, auc:68.7 | loss:0.647
[36,240] | tst acc:61.8, f1:27.4, auc:70.3 | trn acc:66.1, f1:52.0, auc:69.4 | loss:0.638
[37,588] | tst acc:61.4, f1:24.0, auc:69.1 | trn acc:66.4, f1:52.6, auc:69.4 | loss:0.645
[39,260] | tst acc:66.4, f1:30.6, auc:72.9 | trn acc:66.7, f1:52.7, auc:70.0 | loss:0.639
[40,608] | tst acc:64.6, f1:28.9, auc:72.5 | trn acc:67.0, f1:53.2, auc:70.3 | loss:0.631
[42,280] | tst acc:68.8, f1:28.9, auc:72.7 | trn acc:67.2, f1:53.2, auc:70.5 | loss:0.631
[43,628] | tst acc:66.4, f1:29.4, auc:70.9 | trn acc:67.7, f1:53.6, auc:71.0 | loss:0.626
[45,300] | tst acc:64.8, f1:26.5, auc:71.4 | trn acc:68.1, f1:54.1, auc:71.4 | loss:0.628
[46,648] | tst acc:65.9, f1:30.4, auc:71.9 | trn acc:68.4, f1:54.4, auc:71.9 | loss:0.617
[48,320] | tst acc:62.9, f1:25.4, auc:71.5 | trn acc:68.9, f1:54.5, auc:72.2 | loss:0.620
[49,668] | tst acc:67.2, f1:26.5, auc:68.4 | trn acc:69.2, f1:55.2, auc:72.7 | loss:0.607
[51,340] | tst acc:69.9, f1:33.0, auc:77.1 | trn acc:69.6, f1:55.4, auc:73.0 | loss:0.606
[53,12] | tst acc:67.5, f1:29.4, auc:73.4 | trn acc:69.8, f1:55.6, auc:73.3 | loss:0.609
[54,360] | tst acc:64.6, f1:25.0, auc:68.9 | trn acc:70.4, f1:55.9, auc:73.8 | loss:0.602
[56,32] | tst acc:69.2, f1:31.2, auc:73.7 | trn acc:70.6, f1:56.4, auc:74.1 | loss:0.595
[57,380] | tst acc:68.3, f1:31.9, auc:75.4 | trn acc:70.8, f1:56.6, auc:74.3 | loss:0.596
[59,52] | tst acc:67.0, f1:30.4, auc:74.5 | trn acc:71.3, f1:57.1, auc:74.8 | loss:0.590
[60,400] | tst acc:68.6, f1:32.7, auc:77.5 | trn acc:71.7, f1:57.2, auc:75.1 | loss:0.584
[62,72] | tst acc:67.0, f1:30.4, auc:73.3 | trn acc:72.0, f1:57.6, auc:75.4 | loss:0.588
[63,420] | tst acc:66.4, f1:28.7, auc:74.3 | trn acc:72.2, f1:58.1, auc:75.6 | loss:0.581
[65,92] | tst acc:68.6, f1:30.8, auc:74.6 | trn acc:72.7, f1:58.1, auc:76.0 | loss:0.579
[66,440] | tst acc:67.2, f1:29.9, auc:72.7 | trn acc:72.9, f1:58.4, auc:76.2 | loss:0.577
[68,112] | tst acc:71.2, f1:34.7, auc:75.6 | trn acc:73.4, f1:59.1, auc:76.8 | loss:0.565
[69,460] | tst acc:72.3, f1:35.5, auc:77.1 | trn acc:73.4, f1:58.8, auc:76.8 | loss:0.569
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:70.3, f1:30.6, auc:76.4 | trn acc:73.8, f1:59.7, auc:77.1 | loss:0.566
Average epoch runtime: 4.01 seconds
Total training time: 288.50 GPU seconds
SEED: 58475
Loading drug dataset...
Hyperparameters:
Namespace(seed=58475, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='none', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.0001, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=False, alpha=0.6, gamma=2.0, adam=True, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING ADAM OPTIMIZER
Training...
USING NO LR SCHEDULING
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
[0,0] | tst acc:48.3, f1:35.4, auc:49.6 | trn acc:52.6, f1:45.1, auc:53.1 | loss:4.060
[1,348] | tst acc:53.8, f1:41.1, auc:55.1 | trn acc:52.8, f1:40.9, auc:53.6 | loss:3.991
[3,20] | tst acc:52.6, f1:37.9, auc:51.0 | trn acc:53.9, f1:41.7, auc:54.6 | loss:3.564
[4,368] | tst acc:56.7, f1:38.8, auc:57.8 | trn acc:55.1, f1:42.4, auc:56.0 | loss:3.179
[6,40] | tst acc:58.4, f1:43.3, auc:57.5 | trn acc:56.1, f1:43.0, auc:57.0 | loss:2.913
[7,388] | tst acc:59.1, f1:45.5, auc:62.3 | trn acc:57.3, f1:43.9, auc:58.3 | loss:2.671
[9,60] | tst acc:57.5, f1:42.3, auc:58.9 | trn acc:58.2, f1:44.3, auc:59.6 | loss:2.477
[10,408] | tst acc:59.6, f1:42.5, auc:62.7 | trn acc:59.4, f1:45.2, auc:60.6 | loss:2.325
[12,80] | tst acc:58.2, f1:42.4, auc:64.5 | trn acc:60.1, f1:45.9, auc:61.5 | loss:2.201
[13,428] | tst acc:64.7, f1:50.2, auc:65.9 | trn acc:60.8, f1:46.2, auc:62.4 | loss:2.090
[15,100] | tst acc:63.7, f1:50.2, auc:68.9 | trn acc:61.7, f1:47.1, auc:63.3 | loss:1.989
[16,448] | tst acc:65.1, f1:49.5, auc:70.1 | trn acc:62.2, f1:47.5, auc:64.1 | loss:1.872
[18,120] | tst acc:63.0, f1:45.8, auc:66.0 | trn acc:62.8, f1:47.8, auc:64.6 | loss:1.635
[19,468] | tst acc:60.1, f1:42.8, auc:62.9 | trn acc:63.0, f1:48.3, auc:64.4 | loss:0.792
[21,140] | tst acc:64.9, f1:46.7, auc:64.9 | trn acc:63.4, f1:48.1, auc:65.0 | loss:0.672
[22,488] | tst acc:66.1, f1:50.9, auc:68.9 | trn acc:63.5, f1:48.5, auc:65.4 | loss:0.667
[24,160] | tst acc:63.0, f1:46.5, auc:66.9 | trn acc:63.8, f1:48.4, auc:65.5 | loss:0.662
[25,508] | tst acc:66.8, f1:49.3, auc:68.4 | trn acc:64.1, f1:48.7, auc:65.9 | loss:0.663
[27,180] | tst acc:67.8, f1:52.1, auc:69.0 | trn acc:64.4, f1:48.7, auc:66.3 | loss:0.658
[28,528] | tst acc:65.1, f1:48.8, auc:68.8 | trn acc:64.6, f1:49.3, auc:66.5 | loss:0.658
[30,200] | tst acc:63.9, f1:44.0, auc:67.6 | trn acc:65.1, f1:49.5, auc:67.1 | loss:0.654
[31,548] | tst acc:66.1, f1:47.2, auc:68.8 | trn acc:65.3, f1:49.8, auc:67.5 | loss:0.650
[33,220] | tst acc:63.9, f1:43.6, auc:68.9 | trn acc:65.6, f1:50.1, auc:67.7 | loss:0.651
[34,568] | tst acc:68.0, f1:52.3, auc:73.7 | trn acc:66.0, f1:50.3, auc:68.1 | loss:0.640
[36,240] | tst acc:63.7, f1:46.6, auc:68.0 | trn acc:66.4, f1:50.7, auc:68.5 | loss:0.643
[37,588] | tst acc:70.2, f1:55.7, auc:73.7 | trn acc:66.8, f1:51.2, auc:69.1 | loss:0.632
[39,260] | tst acc:68.5, f1:51.3, auc:73.2 | trn acc:66.9, f1:51.2, auc:69.3 | loss:0.636
[40,608] | tst acc:66.8, f1:49.6, auc:68.9 | trn acc:67.3, f1:51.5, auc:69.6 | loss:0.630
[42,280] | tst acc:70.2, f1:52.7, auc:73.3 | trn acc:67.8, f1:51.8, auc:70.3 | loss:0.626
[43,628] | tst acc:67.3, f1:50.4, auc:72.1 | trn acc:68.0, f1:52.1, auc:70.4 | loss:0.624
[45,300] | tst acc:70.9, f1:54.0, auc:75.4 | trn acc:68.5, f1:52.4, auc:71.0 | loss:0.617
[46,648] | tst acc:68.3, f1:48.4, auc:68.6 | trn acc:68.6, f1:52.7, auc:71.1 | loss:0.619
[48,320] | tst acc:71.2, f1:55.6, auc:74.4 | trn acc:68.9, f1:52.7, auc:71.5 | loss:0.615
[49,668] | tst acc:71.9, f1:53.8, auc:74.4 | trn acc:69.5, f1:53.6, auc:72.2 | loss:0.606
[51,340] | tst acc:69.5, f1:53.1, auc:74.7 | trn acc:69.9, f1:53.7, auc:72.6 | loss:0.602
[53,12] | tst acc:68.8, f1:51.1, auc:73.1 | trn acc:70.2, f1:54.3, auc:72.9 | loss:0.602
[54,360] | tst acc:74.3, f1:56.7, auc:74.7 | trn acc:70.6, f1:54.4, auc:73.3 | loss:0.598
[56,32] | tst acc:71.9, f1:55.5, auc:75.6 | trn acc:70.7, f1:54.7, auc:73.4 | loss:0.595
[57,380] | tst acc:70.4, f1:53.9, auc:78.0 | trn acc:71.3, f1:55.3, auc:74.1 | loss:0.588
[59,52] | tst acc:68.3, f1:50.4, auc:73.3 | trn acc:71.5, f1:55.3, auc:74.2 | loss:0.592
[60,400] | tst acc:72.6, f1:55.8, auc:76.3 | trn acc:72.1, f1:56.0, auc:74.8 | loss:0.581
[62,72] | tst acc:72.6, f1:58.1, auc:78.3 | trn acc:72.4, f1:56.2, auc:75.1 | loss:0.582
[63,420] | tst acc:74.3, f1:58.7, auc:78.1 | trn acc:72.7, f1:56.8, auc:75.5 | loss:0.572
[65,92] | tst acc:71.2, f1:52.4, auc:74.1 | trn acc:72.9, f1:56.7, auc:75.8 | loss:0.573
[66,440] | tst acc:75.0, f1:61.2, auc:80.1 | trn acc:72.9, f1:56.8, auc:75.6 | loss:0.574
[68,112] | tst acc:72.8, f1:55.7, auc:76.9 | trn acc:73.6, f1:57.4, auc:76.4 | loss:0.563
[69,460] | tst acc:72.8, f1:55.0, auc:77.6 | trn acc:73.5, f1:57.4, auc:76.3 | loss:0.565
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:70.2, f1:52.7, auc:73.9 | trn acc:73.9, f1:57.6, auc:76.8 | loss:0.561
Average epoch runtime: 4.01 seconds
Total training time: 288.39 GPU seconds
SEED: 94707
Loading drug dataset...
Hyperparameters:
Namespace(seed=94707, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='none', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.0001, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=False, alpha=0.6, gamma=2.0, adam=True, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING ADAM OPTIMIZER
Training...
USING NO LR SCHEDULING
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
[0,0] | tst acc:54.2, f1:55.2, auc:56.6 | trn acc:52.9, f1:45.3, auc:54.3 | loss:4.265
[1,348] | tst acc:52.8, f1:52.7, auc:52.3 | trn acc:51.8, f1:41.1, auc:53.4 | loss:4.207
[3,20] | tst acc:52.0, f1:50.0, auc:52.2 | trn acc:52.9, f1:42.1, auc:54.8 | loss:3.757
[4,368] | tst acc:54.2, f1:54.7, auc:55.2 | trn acc:53.9, f1:42.9, auc:56.3 | loss:3.324
[6,40] | tst acc:55.1, f1:55.0, auc:55.3 | trn acc:55.1, f1:43.5, auc:57.6 | loss:3.010
[7,388] | tst acc:59.0, f1:57.2, auc:58.1 | trn acc:56.1, f1:44.4, auc:58.6 | loss:2.779
[9,60] | tst acc:55.9, f1:54.4, auc:57.9 | trn acc:57.2, f1:45.0, auc:60.0 | loss:2.573
[10,408] | tst acc:59.0, f1:57.5, auc:60.2 | trn acc:58.1, f1:45.7, auc:60.8 | loss:2.422
[12,80] | tst acc:55.9, f1:53.6, auc:59.4 | trn acc:59.0, f1:46.2, auc:61.8 | loss:2.276
[13,428] | tst acc:57.6, f1:53.4, auc:58.8 | trn acc:60.0, f1:46.9, auc:62.7 | loss:2.160
[15,100] | tst acc:57.9, f1:53.6, auc:61.3 | trn acc:60.9, f1:47.6, auc:63.8 | loss:2.041
[16,448] | tst acc:57.3, f1:52.4, auc:62.4 | trn acc:61.4, f1:47.9, auc:64.4 | loss:1.952
[18,120] | tst acc:58.8, f1:54.1, auc:58.0 | trn acc:62.3, f1:48.5, auc:65.1 | loss:1.832
[19,468] | tst acc:57.9, f1:53.9, auc:60.7 | trn acc:62.8, f1:48.6, auc:65.2 | loss:1.358
[21,140] | tst acc:57.6, f1:54.3, auc:61.1 | trn acc:62.9, f1:48.7, auc:65.0 | loss:0.697
[22,488] | tst acc:58.2, f1:52.3, auc:61.7 | trn acc:63.3, f1:49.2, auc:65.5 | loss:0.671
[24,160] | tst acc:59.0, f1:54.3, auc:58.4 | trn acc:63.6, f1:49.2, auc:66.0 | loss:0.665
[25,508] | tst acc:59.3, f1:53.5, auc:61.2 | trn acc:63.8, f1:49.1, auc:66.1 | loss:0.665
[27,180] | tst acc:57.6, f1:51.6, auc:62.4 | trn acc:64.2, f1:49.4, auc:66.5 | loss:0.658
[28,528] | tst acc:59.3, f1:52.9, auc:65.0 | trn acc:64.5, f1:49.6, auc:66.9 | loss:0.655
[30,200] | tst acc:57.1, f1:51.6, auc:63.8 | trn acc:64.8, f1:49.9, auc:67.3 | loss:0.653
[31,548] | tst acc:56.8, f1:50.8, auc:60.8 | trn acc:65.1, f1:50.2, auc:67.7 | loss:0.651
[33,220] | tst acc:60.7, f1:52.9, auc:65.9 | trn acc:65.4, f1:50.3, auc:68.1 | loss:0.650
[34,568] | tst acc:58.8, f1:51.0, auc:62.4 | trn acc:65.9, f1:50.6, auc:68.6 | loss:0.641
[36,240] | tst acc:63.8, f1:58.7, auc:66.3 | trn acc:66.2, f1:50.8, auc:68.8 | loss:0.638
[37,588] | tst acc:61.0, f1:55.8, auc:64.8 | trn acc:66.6, f1:51.3, auc:69.4 | loss:0.637
[39,260] | tst acc:56.8, f1:50.8, auc:61.9 | trn acc:67.0, f1:51.6, auc:69.7 | loss:0.634
[40,608] | tst acc:60.5, f1:53.9, auc:64.0 | trn acc:67.2, f1:51.8, auc:70.1 | loss:0.628
[42,280] | tst acc:59.6, f1:49.8, auc:65.5 | trn acc:67.7, f1:52.2, auc:70.7 | loss:0.621
[43,628] | tst acc:60.2, f1:50.9, auc:65.6 | trn acc:67.9, f1:52.2, auc:70.7 | loss:0.627
[45,300] | tst acc:61.3, f1:55.4, auc:65.3 | trn acc:68.5, f1:52.8, auc:71.4 | loss:0.615
[46,648] | tst acc:60.2, f1:52.5, auc:66.0 | trn acc:68.6, f1:53.0, auc:71.6 | loss:0.620
[48,320] | tst acc:59.0, f1:51.2, auc:65.9 | trn acc:69.1, f1:53.4, auc:72.2 | loss:0.609
[49,668] | tst acc:60.5, f1:52.4, auc:65.3 | trn acc:69.6, f1:53.7, auc:72.5 | loss:0.609
[51,340] | tst acc:59.0, f1:50.5, auc:67.1 | trn acc:69.8, f1:54.0, auc:72.8 | loss:0.600
[53,12] | tst acc:59.9, f1:52.0, auc:67.4 | trn acc:70.5, f1:54.3, auc:73.3 | loss:0.602
[54,360] | tst acc:57.9, f1:48.8, auc:64.7 | trn acc:70.5, f1:54.4, auc:73.4 | loss:0.598
[56,32] | tst acc:61.6, f1:54.4, auc:67.0 | trn acc:70.9, f1:54.9, auc:73.9 | loss:0.590
[57,380] | tst acc:61.9, f1:53.0, auc:67.9 | trn acc:71.3, f1:55.5, auc:74.1 | loss:0.592
[59,52] | tst acc:62.7, f1:53.5, auc:68.9 | trn acc:71.9, f1:55.5, auc:74.9 | loss:0.580
[60,400] | tst acc:61.3, f1:51.6, auc:68.9 | trn acc:72.2, f1:56.1, auc:74.9 | loss:0.584
[62,72] | tst acc:61.6, f1:52.8, auc:69.5 | trn acc:72.4, f1:56.2, auc:75.3 | loss:0.575
[63,420] | tst acc:63.8, f1:53.6, auc:70.3 | trn acc:72.8, f1:56.1, auc:75.5 | loss:0.576
[65,92] | tst acc:60.7, f1:50.2, auc:66.5 | trn acc:73.1, f1:57.1, auc:75.9 | loss:0.569
[66,440] | tst acc:61.6, f1:52.8, auc:67.5 | trn acc:73.3, f1:57.3, auc:76.1 | loss:0.571
[68,112] | tst acc:61.6, f1:51.8, auc:69.7 | trn acc:73.7, f1:57.5, auc:76.4 | loss:0.564
[69,460] | tst acc:62.4, f1:53.3, auc:70.5 | trn acc:73.9, f1:57.7, auc:76.7 | loss:0.563
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:63.3, f1:55.2, auc:69.5 | trn acc:74.1, f1:57.9, auc:77.1 | loss:0.555
Average epoch runtime: 4.01 seconds
Total training time: 288.51 GPU seconds
LR = 0.001 using no SchedulerSEED: 5497
Loading drug dataset...
Hyperparameters:
Namespace(seed=5497, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='none', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.001, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=False, alpha=0.6, gamma=2.0, adam=True, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING ADAM OPTIMIZER
Training...
USING NO LR SCHEDULING
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
[0,0] | tst acc:52.0, f1:21.4, auc:62.4 | trn acc:52.0, f1:41.1, auc:53.9 | loss:4.221
[1,348] | tst acc:55.9, f1:21.7, auc:61.8 | trn acc:56.1, f1:45.1, auc:58.8 | loss:2.843
[3,20] | tst acc:61.4, f1:24.7, auc:67.8 | trn acc:63.2, f1:50.4, auc:63.8 | loss:1.140
[4,368] | tst acc:64.4, f1:31.2, auc:71.9 | trn acc:65.8, f1:52.5, auc:68.9 | loss:0.644
[6,40] | tst acc:66.2, f1:28.6, auc:71.6 | trn acc:68.7, f1:54.5, auc:72.2 | loss:0.619
[7,388] | tst acc:70.3, f1:30.6, auc:72.3 | trn acc:70.8, f1:56.7, auc:74.1 | loss:0.599
[9,60] | tst acc:70.7, f1:35.6, auc:76.9 | trn acc:72.6, f1:58.2, auc:76.1 | loss:0.579
[10,408] | tst acc:69.0, f1:29.7, auc:74.2 | trn acc:74.3, f1:60.0, auc:77.6 | loss:0.561
[12,80] | tst acc:72.9, f1:34.7, auc:75.7 | trn acc:75.4, f1:61.0, auc:78.6 | loss:0.554
[13,428] | tst acc:71.2, f1:31.2, auc:77.0 | trn acc:76.1, f1:61.9, auc:79.2 | loss:0.542
[15,100] | tst acc:75.3, f1:36.2, auc:79.4 | trn acc:76.9, f1:62.4, auc:80.1 | loss:0.524
[16,448] | tst acc:75.1, f1:35.2, auc:78.9 | trn acc:77.6, f1:63.2, auc:81.0 | loss:0.507
[18,120] | tst acc:74.2, f1:35.9, auc:81.3 | trn acc:78.0, f1:63.9, auc:81.7 | loss:0.501
[19,468] | tst acc:74.7, f1:34.1, auc:77.8 | trn acc:78.4, f1:64.2, auc:81.9 | loss:0.495
[21,140] | tst acc:74.0, f1:35.0, auc:79.1 | trn acc:78.7, f1:64.9, auc:82.4 | loss:0.488
[22,488] | tst acc:74.2, f1:36.6, auc:80.0 | trn acc:79.0, f1:64.9, auc:82.6 | loss:0.487
[24,160] | tst acc:75.1, f1:39.4, auc:82.6 | trn acc:79.3, f1:65.7, auc:83.0 | loss:0.481
[25,508] | tst acc:78.4, f1:44.7, auc:82.8 | trn acc:79.4, f1:65.9, auc:83.1 | loss:0.474
[27,180] | tst acc:74.7, f1:37.6, auc:80.6 | trn acc:79.4, f1:65.6, auc:82.9 | loss:0.482
[28,528] | tst acc:76.2, f1:39.1, auc:80.3 | trn acc:79.7, f1:66.2, auc:83.5 | loss:0.472
[30,200] | tst acc:77.3, f1:40.2, auc:81.8 | trn acc:79.7, f1:66.1, auc:83.4 | loss:0.473
[31,548] | tst acc:75.8, f1:38.0, auc:82.2 | trn acc:79.8, f1:66.1, auc:83.5 | loss:0.472
[33,220] | tst acc:77.3, f1:42.2, auc:84.0 | trn acc:79.9, f1:66.6, auc:83.8 | loss:0.469
[34,568] | tst acc:77.7, f1:43.3, auc:83.7 | trn acc:79.9, f1:66.5, auc:83.7 | loss:0.470
[36,240] | tst acc:77.5, f1:41.8, auc:83.5 | trn acc:80.2, f1:66.6, auc:84.0 | loss:0.461
[37,588] | tst acc:76.6, f1:38.9, auc:82.5 | trn acc:80.0, f1:66.6, auc:83.7 | loss:0.471
[39,260] | tst acc:77.7, f1:42.7, auc:83.0 | trn acc:80.2, f1:66.9, auc:84.1 | loss:0.464
[40,608] | tst acc:77.1, f1:39.3, auc:82.1 | trn acc:80.1, f1:66.9, auc:84.1 | loss:0.463
[42,280] | tst acc:77.9, f1:41.6, auc:83.5 | trn acc:80.2, f1:66.9, auc:84.0 | loss:0.465
[43,628] | tst acc:77.3, f1:40.2, auc:82.0 | trn acc:80.2, f1:66.9, auc:84.1 | loss:0.461
[45,300] | tst acc:76.9, f1:41.1, auc:82.6 | trn acc:80.3, f1:67.1, auc:84.0 | loss:0.465
[46,648] | tst acc:77.7, f1:42.0, auc:81.9 | trn acc:80.4, f1:67.3, auc:84.4 | loss:0.457
[48,320] | tst acc:77.7, f1:42.0, auc:83.0 | trn acc:80.6, f1:67.3, auc:84.4 | loss:0.458
[49,668] | tst acc:76.2, f1:38.4, auc:80.2 | trn acc:80.3, f1:67.1, auc:84.3 | loss:0.458
[51,340] | tst acc:79.7, f1:45.0, auc:85.1 | trn acc:80.7, f1:67.7, auc:84.6 | loss:0.453
[53,12] | tst acc:76.6, f1:40.2, auc:81.6 | trn acc:80.3, f1:67.0, auc:84.2 | loss:0.460
[54,360] | tst acc:77.7, f1:42.0, auc:81.7 | trn acc:80.7, f1:67.5, auc:84.6 | loss:0.454
[56,32] | tst acc:76.6, f1:40.2, auc:82.2 | trn acc:80.6, f1:67.6, auc:84.6 | loss:0.454
[57,380] | tst acc:77.1, f1:40.7, auc:82.6 | trn acc:80.5, f1:67.5, auc:84.5 | loss:0.457
[59,52] | tst acc:77.5, f1:41.1, auc:82.7 | trn acc:80.6, f1:67.5, auc:84.7 | loss:0.453
[60,400] | tst acc:76.0, f1:40.2, auc:84.1 | trn acc:80.7, f1:67.6, auc:84.8 | loss:0.451
[62,72] | tst acc:77.1, f1:40.0, auc:81.8 | trn acc:80.8, f1:67.8, auc:84.8 | loss:0.452
[63,420] | tst acc:76.9, f1:41.8, auc:82.5 | trn acc:80.5, f1:67.6, auc:84.6 | loss:0.454
[65,92] | tst acc:78.2, f1:43.2, auc:83.2 | trn acc:80.9, f1:67.8, auc:84.8 | loss:0.449
[66,440] | tst acc:77.5, f1:41.8, auc:82.5 | trn acc:80.7, f1:67.6, auc:84.7 | loss:0.452
[68,112] | tst acc:76.0, f1:38.9, auc:82.4 | trn acc:80.9, f1:68.2, auc:85.2 | loss:0.444
[69,460] | tst acc:77.1, f1:40.0, auc:83.5 | trn acc:80.9, f1:67.8, auc:85.0 | loss:0.448
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:76.0, f1:38.2, auc:82.8 | trn acc:80.8, f1:68.0, auc:84.9 | loss:0.450
Average epoch runtime: 4.01 seconds
Total training time: 289.03 GPU seconds
SEED: 58475
Loading drug dataset...
Hyperparameters:
Namespace(seed=58475, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='none', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.001, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=False, alpha=0.6, gamma=2.0, adam=True, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING ADAM OPTIMIZER
Training...
USING NO LR SCHEDULING
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
[0,0] | tst acc:48.3, f1:35.4, auc:49.7 | trn acc:52.6, f1:45.1, auc:53.1 | loss:4.060
[1,348] | tst acc:64.2, f1:47.7, auc:67.9 | trn acc:57.5, f1:43.8, auc:58.4 | loss:2.806
[3,20] | tst acc:63.7, f1:46.3, auc:64.2 | trn acc:63.8, f1:48.6, auc:63.7 | loss:1.059
[4,368] | tst acc:69.0, f1:49.8, auc:73.9 | trn acc:66.5, f1:50.7, auc:68.6 | loss:0.640
[6,40] | tst acc:69.2, f1:51.1, auc:71.5 | trn acc:68.6, f1:52.6, auc:71.1 | loss:0.619
[7,388] | tst acc:72.1, f1:56.4, auc:76.3 | trn acc:70.8, f1:54.8, auc:73.7 | loss:0.594
[9,60] | tst acc:68.5, f1:48.6, auc:75.6 | trn acc:73.0, f1:56.9, auc:75.8 | loss:0.571
[10,408] | tst acc:73.8, f1:56.6, auc:76.2 | trn acc:74.6, f1:58.6, auc:77.5 | loss:0.557
[12,80] | tst acc:72.8, f1:54.6, auc:79.6 | trn acc:75.4, f1:59.7, auc:78.3 | loss:0.546
[13,428] | tst acc:76.9, f1:60.7, auc:81.0 | trn acc:76.3, f1:60.4, auc:79.2 | loss:0.535
[15,100] | tst acc:75.0, f1:57.0, auc:81.5 | trn acc:77.0, f1:61.2, auc:79.7 | loss:0.534
[16,448] | tst acc:78.4, f1:63.1, auc:82.8 | trn acc:77.4, f1:61.7, auc:80.4 | loss:0.521
[18,120] | tst acc:77.4, f1:60.2, auc:82.3 | trn acc:77.8, f1:62.2, auc:80.6 | loss:0.515
[19,468] | tst acc:74.3, f1:55.2, auc:79.7 | trn acc:78.2, f1:62.8, auc:81.2 | loss:0.512
[21,140] | tst acc:77.4, f1:60.5, auc:80.0 | trn acc:78.4, f1:62.8, auc:81.3 | loss:0.509
[22,488] | tst acc:77.2, f1:60.9, auc:82.3 | trn acc:78.5, f1:63.1, auc:81.7 | loss:0.500
[24,160] | tst acc:79.1, f1:62.7, auc:85.6 | trn acc:78.8, f1:63.4, auc:81.9 | loss:0.492
[25,508] | tst acc:76.9, f1:61.0, auc:83.2 | trn acc:79.1, f1:63.9, auc:82.4 | loss:0.488
[27,180] | tst acc:79.1, f1:63.0, auc:82.6 | trn acc:79.4, f1:64.2, auc:82.8 | loss:0.478
[28,528] | tst acc:79.3, f1:63.6, auc:83.7 | trn acc:79.4, f1:64.5, auc:82.9 | loss:0.480
[30,200] | tst acc:80.0, f1:64.1, auc:83.2 | trn acc:79.7, f1:64.7, auc:83.3 | loss:0.475
[31,548] | tst acc:78.8, f1:62.4, auc:83.9 | trn acc:79.9, f1:65.1, auc:83.6 | loss:0.470
[33,220] | tst acc:77.4, f1:58.0, auc:85.2 | trn acc:79.8, f1:65.1, auc:83.4 | loss:0.473
[34,568] | tst acc:77.6, f1:60.1, auc:85.2 | trn acc:80.0, f1:65.2, auc:83.6 | loss:0.466
[36,240] | tst acc:78.1, f1:60.6, auc:83.5 | trn acc:80.1, f1:65.4, auc:83.7 | loss:0.467
[37,588] | tst acc:79.6, f1:64.4, auc:84.7 | trn acc:80.1, f1:65.5, auc:83.9 | loss:0.461
[39,260] | tst acc:79.3, f1:63.6, auc:85.0 | trn acc:80.1, f1:65.5, auc:83.9 | loss:0.465
[40,608] | tst acc:77.4, f1:60.2, auc:83.3 | trn acc:80.3, f1:65.8, auc:84.0 | loss:0.461
[42,280] | tst acc:80.0, f1:64.4, auc:85.8 | trn acc:80.6, f1:66.2, auc:84.3 | loss:0.458
[43,628] | tst acc:77.9, f1:60.3, auc:84.4 | trn acc:80.1, f1:65.5, auc:84.0 | loss:0.460
[45,300] | tst acc:79.3, f1:62.9, auc:85.3 | trn acc:80.6, f1:66.2, auc:84.4 | loss:0.453
[46,648] | tst acc:78.4, f1:61.2, auc:83.8 | trn acc:80.2, f1:65.8, auc:84.1 | loss:0.461
[48,320] | tst acc:79.8, f1:63.5, auc:85.1 | trn acc:80.4, f1:65.9, auc:84.3 | loss:0.457
[49,668] | tst acc:81.2, f1:66.7, auc:84.4 | trn acc:80.6, f1:66.4, auc:84.6 | loss:0.454
[51,340] | tst acc:82.0, f1:68.4, auc:86.4 | trn acc:80.8, f1:66.4, auc:84.6 | loss:0.450
[53,12] | tst acc:80.8, f1:66.4, auc:85.0 | trn acc:80.6, f1:66.4, auc:84.5 | loss:0.455
[54,360] | tst acc:81.0, f1:65.2, auc:85.3 | trn acc:80.7, f1:66.4, auc:84.7 | loss:0.450
[56,32] | tst acc:79.8, f1:65.3, auc:85.7 | trn acc:80.4, f1:66.1, auc:84.3 | loss:0.456
[57,380] | tst acc:79.8, f1:66.1, auc:86.8 | trn acc:80.8, f1:66.7, auc:84.7 | loss:0.449
[59,52] | tst acc:79.1, f1:62.7, auc:84.4 | trn acc:80.7, f1:66.3, auc:84.6 | loss:0.453
[60,400] | tst acc:80.8, f1:66.1, auc:85.9 | trn acc:80.9, f1:66.7, auc:84.8 | loss:0.448
[62,72] | tst acc:81.2, f1:67.2, auc:86.3 | trn acc:80.9, f1:66.7, auc:84.9 | loss:0.448
[63,420] | tst acc:81.5, f1:67.5, auc:87.0 | trn acc:80.8, f1:66.8, auc:85.0 | loss:0.445
[65,92] | tst acc:80.0, f1:64.7, auc:85.1 | trn acc:80.9, f1:66.5, auc:84.9 | loss:0.446
[66,440] | tst acc:80.0, f1:65.6, auc:87.8 | trn acc:80.7, f1:66.5, auc:84.6 | loss:0.450
[68,112] | tst acc:80.3, f1:65.3, auc:85.4 | trn acc:81.0, f1:67.0, auc:85.2 | loss:0.441
[69,460] | tst acc:81.7, f1:68.3, auc:86.8 | trn acc:80.8, f1:66.8, auc:84.9 | loss:0.447
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:79.8, f1:64.7, auc:84.7 | trn acc:81.3, f1:67.2, auc:85.3 | loss:0.441
Average epoch runtime: 4.01 seconds
Total training time: 288.58 GPU seconds
SEED: 94707
Loading drug dataset...
Hyperparameters:
Namespace(seed=94707, is_train=True, input_dir='data/input', output_dir='data/output', repository='gdsc', drug_id=-1, use_cuda=True, use_relu=True, init_gene_emb=True, scheduler='none', shuffle=False, omic='exp', use_attention=True, use_cntx_attn=True, embedding_dim=200, attention_size=128, attention_head=8, hidden_dim_enc=200, use_hid_lyr=True, max_iter=48000, max_fscore=-1, dropout_rate=0.6, learning_rate=0.001, weight_decay=0.0003, batch_size=8, test_batch_size=8, test_inc_size=1024, model_label='cntx-attn-gdsc', focal=False, alpha=0.6, gamma=2.0, adam=True, exp_size=3000, mut_size=1000, cnv_size=1000, omc_size=3000, drg_size=260, train_size=676, test_size=170)
HIDDEN EMBEDDING SIZE: 200
USING RELU
USING PRETRAING EMBEDDINGS
DROPOUT RATE: 0.6
USING HIDDEN LAYER
USING SELF ATTENTION
USING CONTEXTUAL ATTENTION
INITIALIZING ADAM OPTIMIZER
Training...
USING NO LR SCHEDULING
Batch size: 8
Train dataset size (samples): 676
Batches per epoch: 85
[0,0] | tst acc:54.2, f1:55.2, auc:56.7 | trn acc:52.9, f1:45.3, auc:54.3 | loss:4.265
[1,348] | tst acc:56.2, f1:51.7, auc:58.4 | trn acc:56.3, f1:44.3, auc:58.6 | loss:2.935
[3,20] | tst acc:61.9, f1:55.4, auc:65.3 | trn acc:63.5, f1:49.2, auc:63.9 | loss:1.168
[4,368] | tst acc:60.5, f1:53.6, auc:66.9 | trn acc:66.4, f1:51.1, auc:68.9 | loss:0.640
[6,40] | tst acc:61.9, f1:55.1, auc:65.7 | trn acc:68.9, f1:53.0, auc:71.9 | loss:0.612
[7,388] | tst acc:62.7, f1:54.8, auc:69.1 | trn acc:71.1, f1:55.1, auc:74.0 | loss:0.594
[9,60] | tst acc:63.3, f1:52.2, auc:70.2 | trn acc:73.3, f1:57.1, auc:76.2 | loss:0.568
[10,408] | tst acc:63.3, f1:53.2, auc:70.3 | trn acc:74.7, f1:58.5, auc:77.2 | loss:0.560
[12,80] | tst acc:61.9, f1:49.8, auc:69.6 | trn acc:75.8, f1:59.6, auc:78.4 | loss:0.543
[13,428] | tst acc:63.6, f1:52.7, auc:71.7 | trn acc:76.7, f1:60.9, auc:79.1 | loss:0.538
[15,100] | tst acc:64.7, f1:53.9, auc:71.1 | trn acc:77.3, f1:61.8, auc:80.2 | loss:0.519
[16,448] | tst acc:61.3, f1:49.1, auc:69.8 | trn acc:77.6, f1:61.9, auc:80.4 | loss:0.512
[18,120] | tst acc:60.7, f1:48.7, auc:71.4 | trn acc:78.3, f1:62.9, auc:81.5 | loss:0.496
[19,468] | tst acc:63.8, f1:53.6, auc:72.8 | trn acc:78.8, f1:63.5, auc:81.8 | loss:0.491
[21,140] | tst acc:62.1, f1:49.6, auc:73.2 | trn acc:78.9, f1:63.7, auc:81.9 | loss:0.487
[22,488] | tst acc:62.1, f1:49.6, auc:73.0 | trn acc:79.1, f1:64.1, auc:82.2 | loss:0.484
[24,160] | tst acc:64.1, f1:52.1, auc:71.2 | trn acc:79.6, f1:64.8, auc:82.8 | loss:0.475
[25,508] | tst acc:63.8, f1:53.3, auc:72.9 | trn acc:79.6, f1:64.6, auc:82.8 | loss:0.477
[27,180] | tst acc:62.7, f1:51.5, auc:71.9 | trn acc:79.7, f1:64.8, auc:83.0 | loss:0.472
[28,528] | tst acc:63.8, f1:52.2, auc:75.0 | trn acc:79.9, f1:65.1, auc:83.2 | loss:0.469
[30,200] | tst acc:63.0, f1:52.0, auc:73.7 | trn acc:80.1, f1:65.4, auc:83.5 | loss:0.465
[31,548] | tst acc:64.1, f1:51.7, auc:71.3 | trn acc:80.0, f1:65.4, auc:83.3 | loss:0.470
[33,220] | tst acc:65.0, f1:53.7, auc:74.8 | trn acc:80.2, f1:65.6, auc:83.5 | loss:0.468
[34,568] | tst acc:63.3, f1:50.0, auc:73.4 | trn acc:80.4, f1:65.8, auc:83.7 | loss:0.460
[36,240] | tst acc:64.7, f1:53.2, auc:74.7 | trn acc:80.4, f1:65.8, auc:83.9 | loss:0.458
[37,588] | tst acc:63.3, f1:51.9, auc:73.5 | trn acc:80.2, f1:65.7, auc:83.6 | loss:0.465
[39,260] | tst acc:62.1, f1:50.7, auc:74.0 | trn acc:80.4, f1:65.8, auc:83.8 | loss:0.460
[40,608] | tst acc:63.8, f1:52.2, auc:74.9 | trn acc:80.4, f1:66.0, auc:83.9 | loss:0.459
[42,280] | tst acc:62.4, f1:49.4, auc:73.4 | trn acc:80.7, f1:66.5, auc:84.3 | loss:0.453
[43,628] | tst acc:61.9, f1:49.1, auc:74.6 | trn acc:80.4, f1:65.9, auc:83.9 | loss:0.461
[45,300] | tst acc:63.8, f1:53.3, auc:74.2 | trn acc:80.7, f1:66.4, auc:84.3 | loss:0.452
[46,648] | tst acc:63.6, f1:50.6, auc:74.2 | trn acc:80.6, f1:66.2, auc:84.1 | loss:0.458
[48,320] | tst acc:64.7, f1:53.5, auc:75.7 | trn acc:80.7, f1:66.4, auc:84.4 | loss:0.452
[49,668] | tst acc:63.3, f1:51.1, auc:74.9 | trn acc:80.8, f1:66.3, auc:84.3 | loss:0.453
[51,340] | tst acc:64.4, f1:53.3, auc:74.9 | trn acc:80.6, f1:66.4, auc:84.3 | loss:0.453
[53,12] | tst acc:64.1, f1:53.1, auc:76.5 | trn acc:81.0, f1:66.7, auc:84.5 | loss:0.451
[54,360] | tst acc:62.1, f1:49.6, auc:74.0 | trn acc:80.7, f1:66.3, auc:84.4 | loss:0.452
[56,32] | tst acc:63.3, f1:51.9, auc:75.0 | trn acc:80.8, f1:66.5, auc:84.5 | loss:0.449
[57,380] | tst acc:64.7, f1:53.2, auc:77.2 | trn acc:80.7, f1:66.6, auc:84.4 | loss:0.454
[59,52] | tst acc:66.1, f1:56.2, auc:76.9 | trn acc:81.1, f1:66.7, auc:84.8 | loss:0.443
[60,400] | tst acc:63.0, f1:50.2, auc:75.1 | trn acc:81.0, f1:66.8, auc:84.6 | loss:0.448
[62,72] | tst acc:64.4, f1:54.0, auc:77.4 | trn acc:80.9, f1:66.8, auc:84.8 | loss:0.445
[63,420] | tst acc:64.7, f1:53.9, auc:76.9 | trn acc:81.2, f1:66.6, auc:84.6 | loss:0.445
[65,92] | tst acc:64.4, f1:54.0, auc:76.5 | trn acc:81.0, f1:67.0, auc:84.8 | loss:0.445
[66,440] | tst acc:63.0, f1:50.9, auc:75.5 | trn acc:81.0, f1:67.0, auc:84.7 | loss:0.449
[68,112] | tst acc:64.1, f1:52.4, auc:77.7 | trn acc:81.0, f1:66.7, auc:84.7 | loss:0.444
[69,460] | tst acc:65.3, f1:53.6, auc:77.1 | trn acc:81.0, f1:66.9, auc:84.8 | loss:0.444
Reached final batch of training at iter = 47992
Reached final batch of training at iter = 48000
Epoch 71 finished in 0.05 seconds
[71,4] | tst acc:66.4, f1:56.7, auc:76.5 | trn acc:81.3, f1:67.2, auc:85.2 | loss:0.438
Average epoch runtime: 4.01 seconds
Total training time: 288.49 GPU seconds
